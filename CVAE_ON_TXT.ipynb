{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentences\n",
    "sentences = [\n",
    "    \"hello there\",\n",
    "    \"how are you\",\n",
    "    \"good morning\",\n",
    "    \"see you soon\",\n",
    "    \"what is your name\",\n",
    "    \"i love programming\",\n",
    "    \"where are you from\",\n",
    "    \"nice to meet you\"\n",
    "]\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenized_sentences = [sentence.split() for sentence in sentences]\n",
    "\n",
    "# Create a vocabulary from the unique words in the sentences\n",
    "vocab = set(word for sentence in tokenized_sentences for word in sentence)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Create a word-to-index and index-to-word mapping\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "# Convert sentences to their integer representation\n",
    "encoded_sentences_word_level = [[word2idx[word] for word in sentence] for sentence in tokenized_sentences]\n",
    "\n",
    "vocab_size, encoded_sentences_word_level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def load_and_preprocess_wikitext(file_path, max_sentences=1000):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Use regular expressions to split the text into sentences\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if len(sentence.split()) <= MAX_LENGTH]\n",
    "    \n",
    "    return sentences[:max_sentences]\n",
    "\n",
    "# Adjust the file_path to point to your train.txt from WikiText-2\n",
    "file_path = \"wikitext-2/wiki.train.tokens\"\n",
    "wikitext_sentences = load_and_preprocess_wikitext(file_path)\n",
    "\n",
    "# Print the first few sentences to check\n",
    "wikitext_sentences[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "MAX_LENGTH = 10  # Maximum sentence length\n",
    "VOCAB_SIZE = 50  # For simplicity, let's assume 50 unique tokens (including padding, start, end tokens)\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 256\n",
    "LATENT_DIM = 32  # Dimension of the latent space\n",
    "\n",
    "# Gumbel softmax temperature\n",
    "TAU = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A large team of writers handled the script .',\n",
       " '<unk> can switch classes by changing their assigned weapon .',\n",
       " 'Development work took approximately one year .',\n",
       " 'The newer systems were decided upon early in development .',\n",
       " 'The main color of the Nameless was black .',\n",
       " 'The anime opening was produced by Production I.G.',\n",
       " 'The game was released January 27 , 2011 .',\n",
       " \"He also positively noted the story 's serious tone .\",\n",
       " 'The anime was first announced in November 2010 .',\n",
       " 'Lee of the U.S.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def load_and_preprocess_wikitext(file_path, max_sentences=1000):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Use regular expressions to split the text into sentences\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if len(sentence.split()) <= MAX_LENGTH]\n",
    "    \n",
    "    return sentences[:max_sentences]\n",
    "\n",
    "# Adjust the file_path to point to your train.txt from WikiText-2\n",
    "file_path = \"wikitext-2/wiki.train.tokens\"\n",
    "wikitext_sentences = load_and_preprocess_wikitext(file_path)\n",
    "\n",
    "# Print the first few sentences to check\n",
    "wikitext_sentences[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerCVAE(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (embedding): Embedding(50, 256)\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc_logits): Linear(in_features=256, out_features=32, bias=True)\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (embedding): Embedding(50, 256)\n",
       "    (transformer_decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=50, bias=True)\n",
       "    (fc_z): Linear(in_features=32, out_features=256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the Transformer-based CVAE model\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model=EMBEDDING_DIM, nhead=4, num_layers=2):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, d_model)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead), num_layers\n",
    "        )\n",
    "        self.fc_logits = nn.Linear(d_model, LATENT_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x).permute(1, 0, 2)  # Transformer expects seq_len, batch, features\n",
    "        transformed = self.transformer_encoder(embedded)\n",
    "        # Use the final state to predict logits for latent space\n",
    "        logits = self.fc_logits(transformed[-1])\n",
    "        return logits\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model=EMBEDDING_DIM, nhead=4, num_layers=2):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, d_model)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model, nhead), num_layers\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, VOCAB_SIZE)\n",
    "        self.fc_z = nn.Linear(LATENT_DIM, d_model)  # Convert z to feature size for transformer\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        embedded = self.embedding(x).permute(1, 0, 2)\n",
    "        z_adjusted = self.fc_z(z).unsqueeze(0)\n",
    "        output = self.transformer_decoder(embedded, z_adjusted)\n",
    "        return self.fc_out(output.permute(1, 0, 2))\n",
    "\n",
    "\n",
    "class TransformerCVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerCVAE, self).__init__()\n",
    "        self.encoder = TransformerEncoder()\n",
    "        self.decoder = TransformerDecoder()\n",
    "\n",
    "    def reparameterize(self, logits):\n",
    "        return F.gumbel_softmax(logits, tau=TAU, hard=True, dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.encoder(x)\n",
    "        z = self.reparameterize(logits)\n",
    "        return self.decoder(x, z), logits\n",
    "\n",
    "transformer_cvae = TransformerCVAE()\n",
    "\n",
    "# Check model architecture\n",
    "transformer_cvae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def loss_function(recon_x, x, logits):\n",
    "    recon_loss = F.cross_entropy(recon_x.permute(0, 2, 1), x, reduction='sum')\n",
    "    \n",
    "    # Regularization loss: entropy of the logits\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    reg_loss = -torch.sum(probs * log_probs)\n",
    "    \n",
    "    return recon_loss + reg_loss\n",
    "\n",
    "# Initialize optimizer for the new model\n",
    "optimizer_gumbel = torch.optim.Adam(transformer_cvae.parameters())\n",
    "\n",
    "# 3. Simulate a small dataset\n",
    "num_samples = 100\n",
    "random_sentences = torch.randint(0, VOCAB_SIZE, (num_samples, MAX_LENGTH))\n",
    "\n",
    "# Training loop for the new model\n",
    "def train(epoch, data, model, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i in range(len(data)):\n",
    "        sentence = data[i].unsqueeze(0)\n",
    "        optimizer.zero_grad()\n",
    "        recon_sentence, logits = model(sentence)\n",
    "        loss = loss_function(recon_sentence, sentence, logits)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch: {epoch}, Loss: {train_loss / len(data)}\")\n",
    "\n",
    "# # Train the new model for 10 epochs as a start\n",
    "# for epoch in range(1, 11):\n",
    "#     train(epoch, random_sentences, transformer_cvae, optimizer_gumbel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index to Word Mapping:\n",
      "0: the\n",
      "1: a\n",
      "2: man\n",
      "3: woman\n",
      "4: eats\n",
      "5: reads\n",
      "6: book\n",
      "7: apple\n",
      "8: in\n",
      "9: park\n",
      "10: at\n",
      "11: home\n",
      "12: <PAD>\n",
      "13: <SOS>\n",
      "14: <EOS>\n",
      "\n",
      "First 10 Generated Sentences:\n",
      "<SOS> reads the in <PAD> in park a\n",
      "a man <PAD> book reads apple reads\n",
      "home a eats woman woman in reads man book\n",
      "<SOS> <PAD> a in man woman <SOS> at a <EOS>\n",
      "woman home in <EOS> apple man <SOS>\n",
      "<PAD> at <PAD> a apple\n",
      "in book <PAD> eats eats\n",
      "book book woman book <SOS> eats home home <EOS> <SOS>\n",
      "a park reads at\n",
      "apple the <SOS> in <PAD> park a woman\n"
     ]
    }
   ],
   "source": [
    "# Define a simple vocabulary\n",
    "VOCAB = [\"the\", \"a\", \"man\", \"woman\", \"eats\", \"reads\", \"book\", \"apple\", \"in\", \"park\", \"at\", \"home\", \"<PAD>\", \"<SOS>\", \"<EOS>\"]\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "word2index = {word: idx for idx, word in enumerate(VOCAB)}\n",
    "index2word = {idx: word for word, idx in word2index.items()}\n",
    "\n",
    "# Generate random sentences\n",
    "import random\n",
    "\n",
    "def generate_sentence():\n",
    "    sentence_length = random.randint(3, MAX_LENGTH)\n",
    "    return [random.choice(VOCAB) for _ in range(sentence_length)]\n",
    "\n",
    "def sentence_to_indices(sentence):\n",
    "    return [word2index[word] for word in sentence]\n",
    "\n",
    "NUM_REAL_SAMPLES = 1000\n",
    "real_sentences = [generate_sentence() for _ in range(NUM_REAL_SAMPLES)]\n",
    "real_sentences_indices = [sentence_to_indices(sentence) for sentence in real_sentences]\n",
    "\n",
    "# Pad sentences to max length\n",
    "real_data = torch.full((NUM_REAL_SAMPLES, MAX_LENGTH), word2index[\"<PAD>\"], dtype=torch.long)\n",
    "for i, sentence in enumerate(real_sentences_indices):\n",
    "    real_data[i, :len(sentence)] = torch.LongTensor(sentence)\n",
    "\n",
    "real_data[:5]  # Display first 5 samples\n",
    "\n",
    "# Print each index and its corresponding word\n",
    "index_word_mapping = {index: word for word, index in word2index.items()}\n",
    "\n",
    "print(\"Index to Word Mapping:\")\n",
    "for idx, word in index_word_mapping.items():\n",
    "    print(f\"{idx}: {word}\")\n",
    "\n",
    "# Print first 10 generated sentences\n",
    "print(\"\\nFirst 10 Generated Sentences:\")\n",
    "for sentence in real_sentences[:10]:\n",
    "    print(' '.join(sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.1436982194571174\n",
      "Epoch: 1, Loss: 0.0009616988073394169\n",
      "Epoch: 2, Loss: 0.0003444205750711262\n"
     ]
    }
   ],
   "source": [
    "# Train the Transformer-based CVAE with Gumbel-softmax using the real simulated data\n",
    "\n",
    "# We will train for 5 epochs for demonstration purposes\n",
    "for epoch in range(0, 3):\n",
    "    train(epoch, real_data, transformer_cvae, optimizer_gumbel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def encode(self, sentence):\n",
    "        logits = self.model.encoder(sentence)\n",
    "        z = self.model.reparameterize(logits)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z, sentence_length=MAX_LENGTH):\n",
    "        # Here, we will use a simple greedy decoding method\n",
    "        decoded_sentence = torch.zeros(sentence_length, dtype=torch.long).unsqueeze(0)\n",
    "        for i in range(sentence_length):\n",
    "            logits = self.model.decoder(decoded_sentence, z)\n",
    "            predicted_word = torch.argmax(logits, dim=-1)\n",
    "            decoded_sentence[0, i] = predicted_word[0, i]\n",
    "        return decoded_sentence\n",
    "\n",
    "# Initialize a population of agents\n",
    "NUM_AGENTS = 10\n",
    "agents = [Agent(TransformerCVAE()) for _ in range(NUM_AGENTS)]\n",
    "\n",
    "# Check initialization\n",
    "agents[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interaction_process(agents, data):\n",
    "    total_interactions = 0\n",
    "    successful_interactions = 0\n",
    "    \n",
    "    # For each agent as a sender\n",
    "    for sender in agents:\n",
    "        # Encode a random sentence into a signal\n",
    "        sentence = random.choice(data).unsqueeze(0)\n",
    "        encoded_signal = sender.encode(sentence)\n",
    "        \n",
    "        # All other agents try to decode the signal\n",
    "        for receiver in agents:\n",
    "            if receiver != sender:  # Ensure it's not the same agent\n",
    "                decoded_sentence = receiver.decode(encoded_signal)\n",
    "                if torch.all(decoded_sentence == sentence):\n",
    "                    successful_interactions += 1\n",
    "                total_interactions += 1\n",
    "\n",
    "    return successful_interactions / total_interactions\n",
    "\n",
    "success_rate = interaction_process(agents, real_data)\n",
    "success_rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代理初始化：\n",
    "    我们将初始化一组代理。每个代理都有一个编码器（用于编码消息）和一个解码器（用于解码消息）。\n",
    "交互过程：\n",
    "    在每一代中，随机选择两个代理：一个作为发送者，另一个作为接收者。\n",
    "    发送者选择一个句子并将其编码为一个信号。\n",
    "    接收者尝试解码这个信号。\n",
    "    评估解码的结果，如果接收者正确解码，则交互成功。\n",
    "学习和进化：\n",
    "    基于交互的成功与否，更新代理的编码器和解码器。\n",
    "    代理可能会死亡、繁殖或发生变异，这取决于它们的交互成功率。\n",
    "评估和迭代：\n",
    "    跟踪每一代的成功交互率。\n",
    "    可能会引入新的代理或去除性能较差的代理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the learning function to provide the required inputs to the decoder\n",
    "def learn_from_interaction(sender, receiver, sentence, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Sender encodes the sentence\n",
    "    encoded_signal = sender.encode(sentence)\n",
    "    \n",
    "    # Prepare a partial input for the decoder (starting with <SOS> tokens)\n",
    "    partial_input = torch.full((sentence.shape[0], MAX_LENGTH), word2index[\"<PAD>\"], dtype=torch.long)\n",
    "    partial_input[:, 0] = word2index[\"<SOS>\"]\n",
    "    \n",
    "    # Receiver tries to decode the signal\n",
    "    decoded_sentence_logits = receiver.model.decoder(partial_input, encoded_signal)\n",
    "    \n",
    "    # Calculate loss and update the receiver model\n",
    "    loss = F.cross_entropy(decoded_sentence_logits.permute(0, 2, 1), sentence)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def train_agents_through_interactions(agents, data, epochs=1):\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': agent.model.parameters()} for agent in agents\n",
    "    ])\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # For each agent as a sender\n",
    "        for sender in agents:\n",
    "            # Encode a random sentence into a signal\n",
    "            sentence = random.choice(data).unsqueeze(0)\n",
    "            \n",
    "            # All other agents try to decode the signal\n",
    "            for receiver in agents:\n",
    "                if receiver != sender:  # Ensure it's not the same agent\n",
    "                    learn_from_interaction(sender, receiver, sentence, optimizer)\n",
    "\n",
    "# Train agents through interactions again\n",
    "train_agents_through_interactions(agents, real_data, epochs=1)\n",
    "\n",
    "# Evaluate the success rate after training\n",
    "success_rate_after_training = interaction_process(agents, real_data)\n",
    "success_rate_after_training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolve_agents(agents, data, top_k=2, mutation_rate=0.01):\n",
    "    # 1. Evaluate: calculate success rate for each agent\n",
    "    success_rates = []\n",
    "    for agent in agents:\n",
    "        success_count = 0\n",
    "        for _ in range(10):  # Evaluate each agent over 10 interactions\n",
    "            success_count += interaction_process([agent] + agents, data)\n",
    "        success_rates.append(success_count / 10)\n",
    "    \n",
    "    # 2. Select: Get top performing agents based on success rates\n",
    "    top_agents = [agents[i] for i in sorted(range(len(success_rates)), key=lambda i: success_rates[i], reverse=True)[:top_k]]\n",
    "    \n",
    "    # 3. Reproduce: Clone top agents to replace the low performing ones\n",
    "    for i in range(len(agents) - top_k):\n",
    "        # Copy one of the top agents\n",
    "        new_agent = copy.deepcopy(random.choice(top_agents))\n",
    "        \n",
    "        # 4. Mutate: apply small random noise to the agent's model parameters\n",
    "        for param in new_agent.model.parameters():\n",
    "            if random.random() < mutation_rate:\n",
    "                noise = torch.randn_like(param) * mutation_rate\n",
    "                param.data += noise\n",
    "        \n",
    "        agents[top_k + i] = new_agent\n",
    "    \n",
    "    return agents\n",
    "\n",
    "# Evolve the agents based on their interactions\n",
    "evolved_agents = evolve_agents(agents, real_data)\n",
    "\n",
    "# Check the first evolved agent\n",
    "evolved_agents[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolve_agents_lightweight(agents, data, top_k=2, mutation_rate=0.01):\n",
    "    # 1. Evaluate: calculate success rate for each agent\n",
    "    success_rates = []\n",
    "    for agent in agents:\n",
    "        success_count = 0\n",
    "        for _ in range(10):  # Evaluate each agent over 10 interactions\n",
    "            success_count += interaction_process([agent] + agents, data)\n",
    "        success_rates.append(success_count / 10)\n",
    "    \n",
    "    # 2. Select: Get top performing agents based on success rates\n",
    "    top_agents = [agents[i] for i in sorted(range(len(success_rates)), key=lambda i: success_rates[i], reverse=True)[:top_k]]\n",
    "    \n",
    "    # 3. Reproduce and Mutate: Clone the weights of top agents and apply random noise\n",
    "    for i in range(len(agents) - top_k):\n",
    "        # Copy weights from one of the top agents\n",
    "        chosen_agent = random.choice(top_agents)\n",
    "        new_agent = Agent(chosen_agent.model.__class__())  # Create a new instance of the same model\n",
    "        new_agent.model.load_state_dict(chosen_agent.model.state_dict())  # Load the weights\n",
    "        \n",
    "        # 4. Mutate: apply small random noise to the agent's model parameters\n",
    "        for param in new_agent.model.parameters():\n",
    "            if random.random() < mutation_rate:\n",
    "                noise = torch.randn_like(param) * mutation_rate\n",
    "                param.data += noise\n",
    "        \n",
    "        agents[top_k + i] = new_agent\n",
    "    \n",
    "    return agents\n",
    "\n",
    "# Evolve the agents again using the lightweight method\n",
    "evolved_agents_lightweight = evolve_agents_lightweight(agents, real_data)\n",
    "\n",
    "# Check the first evolved agent\n",
    "evolved_agents_lightweight[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myCVAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
