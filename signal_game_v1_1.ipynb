{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T01:01:13.872529Z",
     "iopub.status.busy": "2023-08-15T01:01:13.872386Z",
     "iopub.status.idle": "2023-08-15T01:01:14.830642Z",
     "shell.execute_reply": "2023-08-15T01:01:14.830068Z",
     "shell.execute_reply.started": "2023-08-15T01:01:13.872511Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T01:01:14.831850Z",
     "iopub.status.busy": "2023-08-15T01:01:14.831697Z",
     "iopub.status.idle": "2023-08-15T01:01:14.861408Z",
     "shell.execute_reply": "2023-08-15T01:01:14.860851Z",
     "shell.execute_reply.started": "2023-08-15T01:01:14.831830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7efe66361f60>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define constants\n",
    "EMBEDDING_DIM = 16\n",
    "HIDDEN_DIM = 16\n",
    "LATENT_DIM = 16 # Dimension of the latent space\n",
    "SEQ_LEN = 16 # Max length of the sequence\n",
    "\n",
    "# Gumbel softmax temperature\n",
    "TAU = 1.0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.random.manual_seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T01:01:14.862823Z",
     "iopub.status.busy": "2023-08-15T01:01:14.862677Z",
     "iopub.status.idle": "2023-08-15T01:01:14.912411Z",
     "shell.execute_reply": "2023-08-15T01:01:14.911634Z",
     "shell.execute_reply.started": "2023-08-15T01:01:14.862805Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pass embeded into decoder instead of using the original x\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model=EMBEDDING_DIM, nhead=4, num_layers=2):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, d_model)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead), num_layers\n",
    "        )\n",
    "        self.fc_logits = nn.Linear(d_model, LATENT_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x).permute(1, 0, 2)  # Transformer expects seq_len, batch, features\n",
    "        transformed = self.transformer_encoder(embedded)\n",
    "        # Use the final state to predict logits for latent space\n",
    "        logits = self.fc_logits(transformed[-1])\n",
    "        return logits, embedded\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model=EMBEDDING_DIM, nhead=4, num_layers=2):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, d_model)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model, nhead), num_layers\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, VOCAB_SIZE)\n",
    "        self.fc_z = nn.Linear(LATENT_DIM, d_model)  # Convert z to feature size for transformer\n",
    "\n",
    "    def forward(self, embedded, z):\n",
    "        # embedded = self.embedding(x).permute(1, 0, 2) # Transformer expects [seq_len, batch, features], permute函数用于改变张量的维度顺序\n",
    "        z_adjusted = self.fc_z(z).unsqueeze(0)\n",
    "        output = self.transformer_decoder(embedded, z_adjusted)\n",
    "        return self.fc_out(output.permute(1, 0, 2))\n",
    "\n",
    "\n",
    "class TransformerCVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerCVAE, self).__init__()\n",
    "        self.encoder = TransformerEncoder()\n",
    "        self.decoder = TransformerDecoder()\n",
    "\n",
    "    def reparameterize(self, logits):\n",
    "        return F.gumbel_softmax(logits, tau=TAU, hard=False, dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits, emb = self.encoder(x)\n",
    "        z = self.reparameterize(logits)\n",
    "        return self.decoder(emb, z), logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T01:01:14.913834Z",
     "iopub.status.busy": "2023-08-15T01:01:14.913689Z",
     "iopub.status.idle": "2023-08-15T01:01:15.198069Z",
     "shell.execute_reply": "2023-08-15T01:01:15.197514Z",
     "shell.execute_reply.started": "2023-08-15T01:01:14.913816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of train sentences:\n",
      "['= Valkyria Chronicles III = \\n \\n Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit .', 'Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable .', 'Released in January 2011 in Japan , it is the third game in the Valkyria series .', '<unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" .', 'The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II .']\n",
      "\n",
      "Sample of test sentences:\n",
      "['= Robert <unk> = \\n \\n Robert <unk> is an English film , television and theatre actor .', 'He had a guest @-@ starring role on the television series The Bill in 2000 .', 'This was followed by a starring role in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre .', 'He had a guest role in the television series Judge John <unk> in 2002 .', 'In 2004 <unk> landed a role as \" Craig \" in the episode \" Teddy \\'s Story \" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi .']\n",
      "\n",
      "Sample of val sentences:\n",
      "['= Homarus gammarus = \\n \\n Homarus gammarus , known as the European lobster or common lobster , is a species of <unk> lobster from the eastern Atlantic Ocean , Mediterranean Sea and parts of the Black Sea .', 'It is closely related to the American lobster , H.', 'americanus .', 'It may grow to a length of 60 cm ( 24 in ) and a mass of 6 kilograms ( 13 lb ) , and bears a conspicuous pair of claws .', 'In life , the lobsters are blue , only becoming \" lobster red \" on cooking .']\n"
     ]
    }
   ],
   "source": [
    "def load_and_preprocess_wikitext(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Use regular expressions to split the text into sentences\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    sentences = [sentence.strip() for sentence in sentences]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "train_file_path = \"wikitext-2/wiki.train.tokens\"\n",
    "test_file_path = \"wikitext-2/wiki.test.tokens\"\n",
    "val_file_path = \"wikitext-2/wiki.valid.tokens\"\n",
    "\n",
    "wikitext_sentences_train = load_and_preprocess_wikitext(train_file_path)\n",
    "wikitext_sentences_test = load_and_preprocess_wikitext(test_file_path)\n",
    "wikitext_sentences_val = load_and_preprocess_wikitext(val_file_path)\n",
    "\n",
    "# Print the first few sentences to check\n",
    "print(\"\\nSample of train sentences:\")\n",
    "print(wikitext_sentences_train[:5])\n",
    "print(\"\\nSample of test sentences:\")\n",
    "print(wikitext_sentences_test[:5])\n",
    "print(\"\\nSample of val sentences:\")\n",
    "print(wikitext_sentences_val[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T01:01:15.199089Z",
     "iopub.status.busy": "2023-08-15T01:01:15.198938Z",
     "iopub.status.idle": "2023-08-15T01:01:16.242699Z",
     "shell.execute_reply": "2023-08-15T01:01:16.242000Z",
     "shell.execute_reply.started": "2023-08-15T01:01:15.199071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 33281\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "# Tokenize the data\n",
    "tokens = [word for sentence in wikitext_sentences_train for word in sentence.split()]\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = [PAD_TOKEN, UNK_TOKEN] + list(set(tokens))\n",
    "word_index = {word: index for index, word in enumerate(vocab)}\n",
    "# 添加新的tokens\n",
    "SOS_TOKEN = '<SOS>'\n",
    "EOS_TOKEN = '<EOS>'\n",
    "word_index[SOS_TOKEN] = len(word_index)\n",
    "word_index[EOS_TOKEN] = len(word_index)\n",
    "vocab = {v: k for k, v in word_index.items()}\n",
    "# Convert tokens to integers\n",
    "def tokenize_and_encode(text):\n",
    "    return [word_index.get(word, word_index[UNK_TOKEN]) for word in text.split()]\n",
    "\n",
    "encoded_data_train = [tokenize_and_encode(sentence) for sentence in wikitext_sentences_train]\n",
    "\n",
    "# Create a PyTorch Dataset\n",
    "class WikiDataset(Dataset):\n",
    "    def __init__(self, data, sequence_length):\n",
    "        self.data = data\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        if len(sample) < self.sequence_length:\n",
    "            sample.extend([word_index[PAD_TOKEN]] * (self.sequence_length - len(sample)))\n",
    "        else:\n",
    "            sample = sample[:self.sequence_length]\n",
    "        return torch.tensor(sample)\n",
    "\n",
    "# dataset = WikiDataset(encoded_data_train, SEQUENCE_LENGTH)\n",
    "# dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# Split the data into train and validation sets\n",
    "dataset = WikiDataset(encoded_data_train, SEQ_LEN)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Display a sample batch\n",
    "next(iter(train_dataloader))\n",
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "print(f'Vocabulary size: {VOCAB_SIZE}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-08-15T01:01:16.244002Z",
     "iopub.status.busy": "2023-08-15T01:01:16.243779Z",
     "iopub.status.idle": "2023-08-15T01:01:18.951619Z",
     "shell.execute_reply": "2023-08-15T01:01:18.951061Z",
     "shell.execute_reply.started": "2023-08-15T01:01:16.243974Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initializing the model with the set hyperparameters\n",
    "transformer_cvae = TransformerCVAE()\n",
    "transformer_cvae.to(device)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(transformer_cvae.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-08-15T01:01:18.953108Z",
     "iopub.status.busy": "2023-08-15T01:01:18.952766Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 | Loss: 1.4324942 = Recon: 1.4281818 + KLD: 0.0004312:  34%|███▎      | 1977/5892 [00:30<03:55, 16.61it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: There were several variants of the <unk> design . ---> Echo: There were several estimated of the <unk> design .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 | Loss: 0.5926737 = Recon: 0.5907571 + KLD: 0.0001917:  66%|██████▌   | 3860/5892 [00:56<00:28, 72.19it/s]"
     ]
    }
   ],
   "source": [
    "def generate_text(model, sentence, vocab):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Tokenize and encode the sentence\n",
    "        encoded_sentence = tokenize_and_encode(sentence)\n",
    "        input_tensor = torch.tensor([encoded_sentence], dtype=torch.long).to(device)  # Ensure type is Long\n",
    "        \n",
    "        # Pass the encoded sentence through the model encoder\n",
    "        with torch.no_grad():\n",
    "            logits, emb = model.encoder(input_tensor)\n",
    "            z = model.reparameterize(logits)\n",
    "            output = model.decoder(emb, z)\n",
    "        \n",
    "        # Convert the output probabilities to predicted token IDs\n",
    "        _, predicted_ids = torch.max(output, dim=2)\n",
    "        predicted_ids = predicted_ids.squeeze().cpu().numpy()\n",
    "        \n",
    "        # Convert the predicted token IDs back to words\n",
    "        predicted_sentence = ' '.join([vocab[idx] for idx in predicted_ids])\n",
    "        \n",
    "        return predicted_sentence\n",
    "\n",
    "def combined_loss_fn(recon_output, target, logits, beta=1.0):\n",
    "    # 计算重构损失\n",
    "    # recon_output = torch.clamp(recon_output, 1e-10, 1 - 1e-10)\n",
    "    recon_loss = F.cross_entropy(recon_output.view(-1, VOCAB_SIZE), target.view(-1), reduction='mean')\n",
    "\n",
    "    # 计算KLD损失\n",
    "    mean, logvar = torch.chunk(logits, 2, dim=-1)\n",
    "    kld_loss = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "\n",
    "    # 计算总损失\n",
    "    total_loss = recon_loss + beta * kld_loss\n",
    "\n",
    "    return total_loss, recon_loss, kld_loss\n",
    "\n",
    "\n",
    "valid_sentence = \"There were several variants of the <unk> design .\"\n",
    "\n",
    "def train_and_visualize(model, train_dataloader, val_dataloader, optimizer, num_epochs, word_index, vocab, beta=10.0):\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    recon_losses = []\n",
    "    kld_losses = []\n",
    "    total_steps = len(train_dataloader) * num_epochs\n",
    "\n",
    "    # Initialize tqdm progress bar\n",
    "    progress_bar = tqdm(total=total_steps, desc=\"Training\", position=0)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_dataloader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output, logits = model(batch)\n",
    "            \n",
    "            # 使用新的损失函数\n",
    "            loss, recon_loss, kld_loss = combined_loss_fn(output, batch, logits, beta)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            recon_losses.append(recon_loss.item())\n",
    "            kld_losses.append(kld_loss.item())\n",
    "            progress_bar.set_description(f'Epoch {epoch+1}/{num_epochs} | Loss: {loss.item():.7f} = Recon: {recon_loss.item():.7f} + KLD: {kld_loss.item():.7f}')\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        # Print epoch loss\n",
    "        avg_loss = sum(losses[-len(train_dataloader):]) / len(train_dataloader)\n",
    "        # print(f\"Done Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_loss:.7f}\")\n",
    "\n",
    "        # Validation after each epoch\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            avg_val_loss = 0\n",
    "            for batch in val_dataloader:\n",
    "                batch = batch.to(device)\n",
    "                output, logits = model(batch)\n",
    "                val_loss, _, _ = combined_loss_fn(output, batch, logits, beta)\n",
    "                val_losses.append(val_loss.item())\n",
    "                avg_val_loss += val_loss.item()\n",
    "            avg_val_loss /= len(val_dataloader)\n",
    "            # print(f\"Done Epoch [{epoch+1}/{num_epochs}] - Validation Loss: {avg_val_loss:.7f}\")\n",
    "\n",
    "        # Generate text after each epoch\n",
    "        generated_sentence = generate_text(model, valid_sentence, vocab)\n",
    "        print(f\"Input: {valid_sentence} ---> Echo: {generated_sentence}\\n\")\n",
    "\n",
    "    # Close the tqdm progress bar\n",
    "    progress_bar.close()\n",
    "    \n",
    "    torch.save(transformer_cvae.state_dict(), 'transformer_cvae.dict')\n",
    "\n",
    "    # Plot the loss curve\n",
    "    plt.plot(losses, label='Total Loss')\n",
    "    plt.plot(recon_losses, label='Reconstruction Loss')\n",
    "    plt.plot(kld_losses, label='KLD Loss')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Curve')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the validation loss curve\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Batches')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Validation Loss Curve')\n",
    "\n",
    "num_epochs = 3\n",
    "train_and_visualize(transformer_cvae, train_dataloader, val_dataloader, optimizer, num_epochs, word_index, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sentence = \"Development work took approximately three years .\"\n",
    "test_generate = generate_text(transformer_cvae, valid_sentence, vocab)\n",
    "test_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize the latent space\n",
    "# from sklearn.manifold import TSNE\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def visualize_latent_space(model, dataloader, device):\n",
    "#     model.eval()\n",
    "    \n",
    "#     # Collect logits from the encoder\n",
    "#     logits_list = []\n",
    "#     with torch.no_grad():\n",
    "#         for batch in dataloader:\n",
    "#             batch = batch.to(device)\n",
    "#             logits, _ = model.encoder(batch)\n",
    "#             logits_list.append(logits)\n",
    "    \n",
    "#     logits_array = torch.cat(logits_list).cpu().numpy()\n",
    "\n",
    "#     # Perform t-SNE\n",
    "#     tsne = TSNE(n_components=2, random_state=1024)\n",
    "#     logits_2d = tsne.fit_transform(logits_array)\n",
    "\n",
    "#     # Plot\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     plt.scatter(logits_2d[:, 0], logits_2d[:, 1], alpha=0.5)\n",
    "#     plt.title(\"t-SNE visualization of logits\")\n",
    "#     plt.show()\n",
    "\n",
    "# # After training, call the visualization function\n",
    "# visualize_latent_space(transformer_cvae, train_dataloader, device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Signal Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, id):\n",
    "        self.sender_model = TransformerEncoder()\n",
    "        self.receive_model = TransformerDecoder()\n",
    "        self.id = id\n",
    "        self.alive = True\n",
    "        self.optimizer = torch.optim.Adam(list(self.sender_model.parameters()) + list(self.receive_model.parameters()), lr=1e-3)\n",
    "\n",
    "    def reparameterize(self, logits):\n",
    "        return F.gumbel_softmax(logits, tau=TAU, hard=False, dim=-1)\n",
    "    \n",
    "    def send(self, msg):\n",
    "        logit, emb = self.sender_model(msg)\n",
    "        z = self.reparameterize(logit)\n",
    "        return emb, z, logit\n",
    "    \n",
    "    def receive(self, emb, z):\n",
    "        decoded_output = self.receive_model(emb, z)\n",
    "        # Convert the output probabilities to predicted token IDs\n",
    "        _, predicted_ids = torch.max(decoded_output, dim=2)\n",
    "        predicted_ids = predicted_ids.squeeze().cpu().numpy()\n",
    "        # Convert the predicted token IDs back to words\n",
    "        decoded_sentence = ' '.join([vocab[idx] for idx in predicted_ids])\n",
    "        return decoded_output, decoded_sentence\n",
    "            \n",
    "\n",
    "def simulation(num_generations=10, initial_population=10):\n",
    "    # population = [Agent(transformer_cvae.to(device), id=i) for i in range(initial_population)]\n",
    "    pretrained_model_path = 'transformer_cvae.dict'\n",
    "    agents = []\n",
    "    for i in range(initial_population):\n",
    "        agents.append(Agent(id=i))\n",
    "    history = []\n",
    "\n",
    "    for generation in range(num_generations):\n",
    "        record = {\n",
    "            'generation': generation,\n",
    "            'population_size': len(agents),\n",
    "            'population_active': sum([agent.alive for agent in agents]),\n",
    "            'avg_influence': sum([agent.influence for agent in agents]) / len(agents),\n",
    "            'successful_communications': 0,\n",
    "            'exact_matches': 0,\n",
    "            'total_communications': 0,\n",
    "            'successful_communication_rate': 0,\n",
    "            'avg_loss': 0,\n",
    "            'communications': [],\n",
    "            'agents_id': [],\n",
    "        }\n",
    "\n",
    "        # Agents communicate\n",
    "        for sender in agents:\n",
    "            for receiver in agents:\n",
    "                if sender.id == receiver.id:\n",
    "                    continue\n",
    "                if not sender.alive or not receiver.alive:\n",
    "                    continue\n",
    "                # idx = torch.randint(0, len(sentences), (1,)).item()\n",
    "                # sentence = sentences[idx]\n",
    "                \n",
    "                batch_sentence = next(iter(train_dataloader))\n",
    "                # Tokenize and encode the sentence\n",
    "                encoded_sentence = tokenize_and_encode(sentence)\n",
    "                input_tensor = torch.tensor([encoded_sentence], dtype=torch.long).to(device)\n",
    "                \n",
    "                with autocast():  # Enable autocast for FP16\n",
    "                    # Sender sends the message\n",
    "                    emb, z, logit = sender.send(input_tensor)\n",
    "                    # Receiver receives the message\n",
    "                    output_tensor, decoded_sentence = receiver.receive(emb, z)\n",
    "                    # Calculate the loss\n",
    "                    loss,_,_ = combined_loss_fn(output_tensor, input_tensor, logit, beta=0.0)\n",
    "\n",
    "                # Use the scaler to scale the loss and back-propagate the scaled gradients\n",
    "                scaler.scale(loss).backward()\n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(sender.sender_model.parameters(), 1.0)\n",
    "                torch.nn.utils.clip_grad_norm_(receiver.receive_model.parameters(), 1.0)\n",
    "                # Using the agent's own optimizer\n",
    "                scaler.step(sender.sender_model)\n",
    "                scaler.step(receiver.receive_model)\n",
    "                scaler.update()\n",
    "                record['total_communications'] += 1\n",
    "                if loss.item() < 0.5:\n",
    "                    record['successful_communications'] += 1\n",
    "                record['avg_loss'] += loss.item()\n",
    "                record['communications'].append({\n",
    "                    'sender': sender.id,\n",
    "                    'receiver': receiver.id,\n",
    "                    'original_sentence': sentence,\n",
    "                    'decoded_sentence': decoded_sentence,\n",
    "                    'loss': loss.item(),\n",
    "                })\n",
    "                agnet_avg_loss += loss.item()\n",
    "                print(f'agent:{sender.id},loss:{loss.item()}')\n",
    "\n",
    "        record['avg_loss'] /= record['total_communications']\n",
    "        print(f\"Gen[{generation+1}/{num_generations}] | Suc. Rate: {record['successful_communications']/record['total_communications']} | Avg. Loss: {record['avg_loss']}\")\n",
    "        history.append(record)\n",
    "\n",
    "    return history, agents\n",
    "\n",
    "    return history\n",
    "\n",
    "# 提取句子列表\n",
    "sentences = []\n",
    "for batch in train_dataloader:\n",
    "    for seq in batch:\n",
    "        sentence = ' '.join([vocab[idx.item()] for idx in seq])\n",
    "        sentences.append(sentence)\n",
    "# Call the simulation function\n",
    "history = simulation(num_generations=5, initial_population=5)\n",
    "\n",
    "# Print the history records\n",
    "for record in history:\n",
    "    print(f\"Generation: {record['generation']}\")\n",
    "    print(f\"Agents: {record['agents_id']}\")\n",
    "    print(f\"Population Size: {record['population_size']}\")\n",
    "    print(f\"Average Influence: {record['avg_influence']:.2f}\")\n",
    "    print(f\"Successful Communications: {record['successful_communications']}/{record['total_communications']}\")\n",
    "    print(f\"Successful Communication Rate: {record['successful_communication_rate']:.7f}\")\n",
    "    print(f\"Exact_Matches: {record['exact_matches']}\")\n",
    "    print(f\"Average Communication Loss: {record['avg_loss']:.7f}\")\n",
    "    # for comm in record['communications']:\n",
    "    #     print(f\"  Agent {comm['sender']} -> Agent {comm['receiver']}:\")\n",
    "    #     print(f\"    Original Sentence: {comm['original_sentence']}\")\n",
    "    #     print(f\"    Decoded Sentence: {comm['decoded_sentence']}\")\n",
    "    #     print(f\"    Successful: {comm['successful']}\")\n",
    "    #     print(f\"    Loss: {comm['loss']:.7f}\")\n",
    "    print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the history records\n",
    "for record in history:\n",
    "    print(f\"Generation: {record['generation']}\")\n",
    "    print(f\"Population Size: {record['population_size']}\")\n",
    "    print(f\"Average Influence: {record['avg_influence']:.2f}\")\n",
    "    print(f\"Successful Communications: {record['successful_communications']}/{record['total_communications']}\")\n",
    "    print(f\"Successful Communication Rate: {record['successful_communication_rate']:.7f}\")\n",
    "    print(f\"Average Communication Loss: {record['avg_loss']:.7f}\")\n",
    "    for comm in record['communications']:\n",
    "        print(f\"  Agent {comm['sender']} -> Agent {comm['receiver']}:\")\n",
    "        print(f\"    Exact Match: {comm['exact_match']}\")\n",
    "        print(f\"    Original Sentence: {comm['original_sentence']}\")\n",
    "        print(f\"    Decoded Sentence: {comm['decoded_sentence']}\")\n",
    "        print(f\"    Successful: {comm['successful']}\")\n",
    "        print(f\"    Loss: {comm['loss']:.7f}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sentence = \"This included extensive evidence for the production of high status jewellery and moulds from the seventh\"\n",
    "test_generate = generate_text(transformer_cvae, valid_sentence, vocab)\n",
    "test_generate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "平均适应度变化:\n",
    "\n",
    "记录所有agents的适应度平均值，并在每个周期后绘制它。这将为您提供一个关于整体群体如何进化的视图。\n",
    "\n",
    "python\n",
    "Copy code\n",
    "avg_fitness = sum([agent.fitness for agent in agents]) / len(agents)\n",
    "最佳和最差适应度变化:\n",
    "\n",
    "跟踪并可视化最佳和最差适应度的agents。这有助于了解适应度分布的范围如何随时间变化。\n",
    "\n",
    "模型权重分布:\n",
    "\n",
    "为了了解权重如何传播，您可以使用某种可视化工具（如PCA或t-SNE）来可视化agent模型权重的分布。如果你看到权重聚集在某些区域，这可能意味着某些特定的模型结构正在变得主导。\n",
    "\n",
    "种群大小:\n",
    "\n",
    "如果你允许agents繁殖，那么绘制种群大小作为时间函数可能会很有趣。这有助于你了解种群是如何增长或收缩的，以及是否有任何爆炸性增长或大量死亡。\n",
    "\n",
    "交互效果示例:\n",
    "\n",
    "随机选择一些句子并展示agent如何对它们进行编码和解码。这有助于定性地了解agents的性能。\n",
    "\n",
    "适应度分布直方图:\n",
    "\n",
    "在每个周期或几个周期后，您可以为所有agents的适应度绘制一个直方图，以查看适应度如何分布。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模拟实时展示:\n",
    "a. 动态代表每个Agent:\n",
    "使用小圆圈或其他形状代表每个agent。\n",
    "使用不同的颜色或大小来表示agent的适应度或权重。\n",
    "b. 实时交互:\n",
    "当agent互相交流时，显示连线或动画效果。\n",
    "当新的agent生成或某个agent消失时，显示出现或消失的动画效果。\n",
    "c. 时间控制:\n",
    "允许用户暂停、开始或加速模拟。\n",
    "d. 参数调整:\n",
    "提供滑块或输入框，让用户调整模拟参数，如初始agent数量、交流频率、权重传播速率等。\n",
    "2. 交互式可视化:\n",
    "a. 数据视图切换:\n",
    "允许用户在不同的视图之间切换，例如适应度曲线、权重分布图、种群大小随时间变化的图等。\n",
    "b. 工具提示和详细信息:\n",
    "当用户将鼠标悬停在某个数据点或agent上时，显示详细的信息或统计数据。\n",
    "c. 参数调整:\n",
    "提供控件，允许用户修改可视化的参数。例如，查看在不同权重传播策略下的适应度曲线。\n",
    "d. 交互式教程:\n",
    "为用户提供一个交互式教程，引导他们了解如何使用可视化工具，以及他们可以从中学到什么。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
