{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A large team of writers handled the script .',\n",
       " '<unk> can switch classes by changing their assigned weapon .',\n",
       " 'Development work took approximately one year .',\n",
       " 'The newer systems were decided upon early in development .',\n",
       " 'The main color of the Nameless was black .',\n",
       " 'The anime opening was produced by Production I.G.',\n",
       " 'The game was released January 27 , 2011 .',\n",
       " \"He also positively noted the story 's serious tone .\",\n",
       " 'The anime was first announced in November 2010 .',\n",
       " 'Lee of the U.S.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_and_preprocess_wikitext(file_path, max_sentences=1000):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Use regular expressions to split the text into sentences\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if len(sentence.split()) <= MAX_LENGTH]\n",
    "    \n",
    "    return sentences[:max_sentences]\n",
    "\n",
    "\n",
    "MAX_LENGTH = 10\n",
    "max_sentences = 1000\n",
    "file_path = \"wikitext-2/wiki.train.tokens\"\n",
    "wikitext_sentences = load_and_preprocess_wikitext(file_path, max_sentences=max_sentences)\n",
    "\n",
    "# Print the first few sentences to check\n",
    "wikitext_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "MAX_LENGTH = 10  # Maximum sentence length\n",
    "VOCAB_SIZE = 50  # For simplicity, let's assume 50 unique tokens (including padding, start, end tokens)\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 256\n",
    "LATENT_DIM = 32  # Dimension of the latent space\n",
    "\n",
    "# Gumbel softmax temperature\n",
    "TAU = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Transformer-based CVAE model\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model=EMBEDDING_DIM, nhead=4, num_layers=2):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, d_model)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead), num_layers\n",
    "        )\n",
    "        self.fc_logits = nn.Linear(d_model, LATENT_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x).permute(1, 0, 2)  # Transformer expects seq_len, batch, features\n",
    "        transformed = self.transformer_encoder(embedded)\n",
    "        # Use the final state to predict logits for latent space\n",
    "        logits = self.fc_logits(transformed[-1])\n",
    "        return logits\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model=EMBEDDING_DIM, nhead=4, num_layers=2):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, d_model)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model, nhead), num_layers\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, VOCAB_SIZE)\n",
    "        self.fc_z = nn.Linear(LATENT_DIM, d_model)  # Convert z to feature size for transformer\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        embedded = self.embedding(x).permute(1, 0, 2)\n",
    "        z_adjusted = self.fc_z(z).unsqueeze(0)\n",
    "        output = self.transformer_decoder(embedded, z_adjusted)\n",
    "        return self.fc_out(output.permute(1, 0, 2))\n",
    "\n",
    "\n",
    "class TransformerCVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerCVAE, self).__init__()\n",
    "        self.encoder = TransformerEncoder()\n",
    "        self.decoder = TransformerDecoder()\n",
    "\n",
    "    def reparameterize(self, logits):\n",
    "        return F.gumbel_softmax(logits, tau=TAU, hard=True, dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.encoder(x)\n",
    "        z = self.reparameterize(logits)\n",
    "        return self.decoder(x, z), logits\n",
    "\n",
    "transformer_cvae = TransformerCVAE()\n",
    "\n",
    "# # Check model architecture\n",
    "# transformer_cvae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def loss_function(recon_x, x, logits):\n",
    "    recon_loss = F.cross_entropy(recon_x.permute(0, 2, 1), x, reduction='sum')\n",
    "    \n",
    "    # Regularization loss: entropy of the logits\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    reg_loss = -torch.sum(probs * log_probs)\n",
    "    \n",
    "    return recon_loss + reg_loss\n",
    "\n",
    "# Initialize optimizer for the new model\n",
    "optimizer_gumbel = torch.optim.Adam(transformer_cvae.parameters())\n",
    "\n",
    "# Training loop for the new model\n",
    "def train(epoch, data, model, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i in range(len(data)):\n",
    "        sentence = data[i].unsqueeze(0)\n",
    "        optimizer.zero_grad()\n",
    "        recon_sentence, logits = model(sentence)\n",
    "        loss = loss_function(recon_sentence, sentence, logits)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch: {epoch}, Loss: {train_loss / len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' \\n',\n",
       " ' = Valkyria Chronicles III = \\n',\n",
       " ' \\n',\n",
       " ' Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \\n',\n",
       " \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more <unk> for series newcomers . Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the provided wiki dataset\n",
    "with open(\"wikitext-2/wiki.train.tokens\", \"r\", encoding=\"utf-8\") as file:\n",
    "    wiki_data = file.readlines()\n",
    "\n",
    "# Display the first few lines of the dataset\n",
    "wiki_data[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  11,   11,    1,   11,   11,    0,    0,    0,    0,    0],\n",
       "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,   44,    1,   44,    1,   19,    2,    1,    1,    5],\n",
       "        [   6,    1,   13,  499,    8,  241, 1880,   22,    2,    1],\n",
       "        [ 673,   48,    1,    9,  150,    1,    3,    1,   13,    1],\n",
       "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [  14,   83,    1,    1,    1,    1,   13,  486,   22,    2],\n",
       "        [ 425,   51,  809,    1,    3,  433,    8,    2, 1181,    5],\n",
       "        [   6,    3,    2,    1,   25,  218,    1,    7,  968,   85],\n",
       "        [  11,   11,   11,   11,  243,  281,   11,   11,   11,   11],\n",
       "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [  14,    6,   37,   10,  357,    1,  244,   21,  835,   24],\n",
       "        [  14,  320,   18,  273,    3,   10,    1,    5,  373,  328],\n",
       "        [  11,   11,   11,    6,  130,   11,   11,   11,    0,    0],\n",
       "        [  11,   11,   11,    1,   11,   11,   11,    0,    0,    0],\n",
       "        [  11,   11,  886,  817,   11,   11,    0,    0,    0,    0],\n",
       "        [  11,   11,    6,   11,   11,    0,    0,    0,    0,    0],\n",
       "        [  11,   11,   11,   11,    1,   11,   11,   11,   11,    0],\n",
       "        [  14, 1806,    1,    7,    2, 1861,    7,  922,    1,  867],\n",
       "        [ 991,    2, 1190,    1,    5,    1,    3,    2,  615,  871],\n",
       "        [  11,   11,   11,    1,   11,   11,   11,    0,    0,    0],\n",
       "        [1880,    3,  572,    3,    1,    1,   24,    1,    6,   23],\n",
       "        [1206,  105,   61,    1,   18,  290,    1, 1151,  774,    6],\n",
       "        [ 312,    1,   25,   16,   70,   10,    1,  902, 1701,    3],\n",
       "        [   1,    1,   26,   10,  377, 1769,   16,  293,  189,    2],\n",
       "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [  11,   11,    1,   11,   11,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Hyperparameters\n",
    "MAX_VOCAB_SIZE = 2000\n",
    "BATCH_SIZE = 32\n",
    "SEQUENCE_LENGTH = 10\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "# Tokenize the data\n",
    "tokens = [word for sentence in wiki_data for word in sentence.split()]\n",
    "\n",
    "# Build vocabulary\n",
    "word_counts = Counter(tokens)\n",
    "vocab = [PAD_TOKEN, UNK_TOKEN] + [word for word, count in word_counts.most_common(MAX_VOCAB_SIZE - 2)]\n",
    "word_to_index = {word: index for index, word in enumerate(vocab)}\n",
    "\n",
    "# Convert tokens to integers\n",
    "def tokenize_and_encode(text):\n",
    "    return [word_to_index.get(word, word_to_index[UNK_TOKEN]) for word in text.split()]\n",
    "\n",
    "encoded_data = [tokenize_and_encode(sentence) for sentence in wiki_data]\n",
    "\n",
    "# Create a PyTorch Dataset\n",
    "class WikiDataset(Dataset):\n",
    "    def __init__(self, data, sequence_length):\n",
    "        self.data = data\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        if len(sample) < self.sequence_length:\n",
    "            sample.extend([word_to_index[PAD_TOKEN]] * (self.sequence_length - len(sample)))\n",
    "        else:\n",
    "            sample = sample[:self.sequence_length]\n",
    "        return torch.tensor(sample)\n",
    "\n",
    "dataset = WikiDataset(encoded_data, SEQUENCE_LENGTH)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Display a sample batch\n",
    "next(iter(dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerCVAE(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (embedding): Embedding(2000, 256)\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc_logits): Linear(in_features=256, out_features=16, bias=True)\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (embedding): Embedding(2000, 256)\n",
       "    (transformer_decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=2000, bias=True)\n",
       "    (fc_z): Linear(in_features=16, out_features=256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the hyperparameters for the model based on the processed data\n",
    "EMBEDDING_DIM = 32\n",
    "VOCAB_SIZE = len(vocab)\n",
    "LATENT_DIM = 16\n",
    "TAU = 0.5\n",
    "\n",
    "# Initializing the model with the set hyperparameters\n",
    "transformer_cvae = TransformerCVAE()\n",
    "\n",
    "# Check if GPU is available and move the model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transformer_cvae.to(device)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(transformer_cvae.parameters(), lr=0.001)\n",
    "\n",
    "# Display model architecture\n",
    "transformer_cvae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Loss: 0.0013660: 100%|██████████| 1148/1148 [06:25<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1] - Loss: 0.1483\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Redefine the loss function and the optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(transformer_cvae.parameters(), lr=0.001)\n",
    "num_epochs = 1\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "total_steps = len(dataloader) * num_epochs\n",
    "\n",
    "# Initialize tqdm progress bar\n",
    "progress_bar = tqdm(total=total_steps, desc=\"Training\", position=0)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, logits = transformer_cvae(batch)\n",
    "        loss = criterion(output.view(-1, VOCAB_SIZE), batch.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        progress_bar.set_description(f'Epoch {epoch+1}/{num_epochs} | Loss: {loss.item():.7f}')\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    # Print epoch loss\n",
    "    avg_loss = sum(losses[-len(dataloader):]) / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Close the tqdm progress bar\n",
    "progress_bar.close()\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Loss: 0.0004164: 100%|██████████| 1148/1148 [06:56<00:00,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/1] - Loss: 0.0006\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[1;32m     67\u001b[0m \u001b[39m# Now, let's call the modified training loop\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m train_and_visualize(transformer_cvae, dataloader, optimizer, criterion, num_epochs, word_to_index, vocab, \u001b[39m\"\u001b[39;49m\u001b[39mwikitext-2/wiki.valid.tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[10], line 53\u001b[0m, in \u001b[0;36mtrain_and_visualize\u001b[0;34m(model, dataloader, optimizer, criterion, num_epochs, word_to_index, vocab, valid_file_path)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mEpoch [\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m] - Loss: \u001b[39m\u001b[39m{\u001b[39;00mavg_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[39m# Generate text after each epoch\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m generated_sentence \u001b[39m=\u001b[39m generate_text(model, valid_sentence, word_to_index, vocab)\n\u001b[1;32m     54\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOriginal Sentence: \u001b[39m\u001b[39m{\u001b[39;00mvalid_sentence\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGenerated Sentence: \u001b[39m\u001b[39m{\u001b[39;00mgenerated_sentence\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m, in \u001b[0;36mgenerate_text\u001b[0;34m(model, sentence, word_to_index, vocab, max_length)\u001b[0m\n\u001b[1;32m      7\u001b[0m input_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([encoded_sentence])\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m \u001b[39m# Pass the encoded sentence through the model\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m output, _ \u001b[39m=\u001b[39m model(input_tensor)\n\u001b[1;32m     12\u001b[0m \u001b[39m# Convert the output probabilities to predicted token IDs\u001b[39;00m\n\u001b[1;32m     13\u001b[0m _, predicted_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(output, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[4], line 46\u001b[0m, in \u001b[0;36mTransformerCVAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 46\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[1;32m     47\u001b[0m     z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreparameterize(logits)\n\u001b[1;32m     48\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(x, z), logits\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 12\u001b[0m     embedded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding(x)\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# Transformer expects seq_len, batch, features\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     transformed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_encoder(embedded)\n\u001b[1;32m     14\u001b[0m     \u001b[39m# Use the final state to predict logits for latent space\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "# Define a function to generate text using the model\n",
    "def generate_text(model, sentence, word_to_index, vocab, max_length=SEQUENCE_LENGTH):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Tokenize and encode the sentence\n",
    "        encoded_sentence = tokenize_and_encode(sentence)\n",
    "        input_tensor = torch.tensor([encoded_sentence]).to(device)\n",
    "        \n",
    "        # Pass the encoded sentence through the model\n",
    "        output, _ = model(input_tensor)\n",
    "        \n",
    "        # Convert the output probabilities to predicted token IDs\n",
    "        _, predicted_ids = torch.max(output, dim=2)\n",
    "        predicted_ids = predicted_ids.squeeze().cpu().numpy()\n",
    "        \n",
    "        # Convert the predicted token IDs back to words\n",
    "        predicted_sentence = ' '.join([vocab[idx] for idx in predicted_ids])\n",
    "        \n",
    "        return predicted_sentence\n",
    "\n",
    "# Modify the training loop to print out a generated sentence at the end of each epoch\n",
    "def train_and_visualize(model, dataloader, optimizer, criterion, num_epochs, word_to_index, vocab, valid_file_path):\n",
    "    # Load a validation sentence\n",
    "    with open(valid_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        valid_data = file.readlines()\n",
    "    \n",
    "    # For simplicity, just use the first sentence from the validation set for visualization\n",
    "    valid_sentence = valid_data[0].strip()\n",
    "    \n",
    "    losses = []\n",
    "    total_steps = len(dataloader) * num_epochs\n",
    "\n",
    "    # Initialize tqdm progress bar\n",
    "    progress_bar = tqdm(total=total_steps, desc=\"Training\", position=0)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output, logits = model(batch)\n",
    "            loss = criterion(output.view(-1, VOCAB_SIZE), batch.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            progress_bar.set_description(f'Epoch {epoch+1}/{num_epochs} | Loss: {loss.item():.7f}')\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        # Print epoch loss\n",
    "        avg_loss = sum(losses[-len(dataloader):]) / len(dataloader)\n",
    "        print(f\"\\nEpoch [{epoch+1}/{num_epochs}] - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Generate text after each epoch\n",
    "        generated_sentence = generate_text(model, valid_sentence, word_to_index, vocab)\n",
    "        print(f\"Original Sentence: {valid_sentence}\")\n",
    "        print(f\"Generated Sentence: {generated_sentence}\\n\")\n",
    "\n",
    "    # Close the tqdm progress bar\n",
    "    progress_bar.close()\n",
    "\n",
    "    # Plot the loss curve\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Curve')\n",
    "    plt.show()\n",
    "\n",
    "# Now, let's call the modified training loop\n",
    "train_and_visualize(transformer_cvae, dataloader, optimizer, criterion, num_epochs, word_to_index, vocab, \"wikitext-2/wiki.valid.tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of logits: torch.Size([1, 16])\n",
      "<UNK> <UNK> back <UNK>\n"
     ]
    }
   ],
   "source": [
    "# Modify the generate_text_debug function to handle the outputs correctly\n",
    "\n",
    "def generate_text_debug_v2(model, sentence, word_to_index, vocab, max_length=10):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Tokenize and encode the sentence\n",
    "        encoded_sentence = tokenize_and_encode(sentence)\n",
    "        input_tensor = torch.tensor([encoded_sentence], dtype=torch.long).to(device)  # Ensure type is Long\n",
    "        \n",
    "        # Pass the encoded sentence through the model encoder\n",
    "        logits = model.encoder(input_tensor)\n",
    "        print(f\"Shape of logits: {logits.shape}\")\n",
    "        \n",
    "        z = model.reparameterize(logits)\n",
    "        output = model.decoder(input_tensor, z)\n",
    "        \n",
    "        # Convert the output probabilities to predicted token IDs\n",
    "        _, predicted_ids = torch.max(output, dim=2)\n",
    "        predicted_ids = predicted_ids.squeeze().cpu().numpy()\n",
    "        \n",
    "        # Convert the predicted token IDs back to words\n",
    "        predicted_sentence = ' '.join([vocab[idx] for idx in predicted_ids])\n",
    "        \n",
    "        return predicted_sentence\n",
    "\n",
    "# Call the debug function\n",
    "valid_sentence = \"Bring yourself back online.\"\n",
    "generated_sentence_debug_v2 = generate_text_debug_v2(transformer_cvae, valid_sentence, word_to_index, vocab)\n",
    "print(generated_sentence_debug_v2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myCVAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
