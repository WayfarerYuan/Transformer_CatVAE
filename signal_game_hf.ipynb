{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x113efd8b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define constants\n",
    "EMBEDDING_DIM = 16\n",
    "HIDDEN_DIM = 16\n",
    "LATENT_DIM = 16 # Dimension of the latent space\n",
    "SEQ_LEN = 16 # Max length of the sequence\n",
    "\n",
    "# Gumbel softmax temperature\n",
    "TAU = 1.0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.random.manual_seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass embeded into decoder instead of using the original x\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model=EMBEDDING_DIM, nhead=4, num_layers=2):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, d_model)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead), num_layers\n",
    "        )\n",
    "        self.fc_logits = nn.Linear(d_model, LATENT_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x).permute(1, 0, 2)  # Transformer expects seq_len, batch, features\n",
    "        transformed = self.transformer_encoder(embedded)\n",
    "        # Use the final state to predict logits for latent space\n",
    "        logits = self.fc_logits(transformed[-1])\n",
    "        return logits, embedded\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model=EMBEDDING_DIM, nhead=4, num_layers=2):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, d_model)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model, nhead), num_layers\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, VOCAB_SIZE)\n",
    "        self.fc_z = nn.Linear(LATENT_DIM, d_model)  # Convert z to feature size for transformer\n",
    "\n",
    "    def forward(self, embedded, z):\n",
    "        # embedded = self.embedding(x).permute(1, 0, 2) # Transformer expects [seq_len, batch, features], permute函数用于改变张量的维度顺序\n",
    "        z_adjusted = self.fc_z(z).unsqueeze(0)\n",
    "        output = self.transformer_decoder(embedded, z_adjusted)\n",
    "        return self.fc_out(output.permute(1, 0, 2))\n",
    "\n",
    "\n",
    "class TransformerCVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerCVAE, self).__init__()\n",
    "        self.encoder = TransformerEncoder()\n",
    "        self.decoder = TransformerDecoder()\n",
    "\n",
    "    def reparameterize(self, logits):\n",
    "        return F.gumbel_softmax(logits, tau=TAU, hard=False, dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits, emb = self.encoder(x)\n",
    "        z = self.reparameterize(logits)\n",
    "        return self.decoder(emb, z), logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of train sentences:\n",
      "['= Valkyria Chronicles III = \\n \\n Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit .', 'Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable .', 'Released in January 2011 in Japan , it is the third game in the Valkyria series .', '<unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" .', 'The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II .']\n",
      "\n",
      "Sample of test sentences:\n",
      "['= Robert <unk> = \\n \\n Robert <unk> is an English film , television and theatre actor .', 'He had a guest @-@ starring role on the television series The Bill in 2000 .', 'This was followed by a starring role in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre .', 'He had a guest role in the television series Judge John <unk> in 2002 .', 'In 2004 <unk> landed a role as \" Craig \" in the episode \" Teddy \\'s Story \" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi .']\n",
      "\n",
      "Sample of val sentences:\n",
      "['= Homarus gammarus = \\n \\n Homarus gammarus , known as the European lobster or common lobster , is a species of <unk> lobster from the eastern Atlantic Ocean , Mediterranean Sea and parts of the Black Sea .', 'It is closely related to the American lobster , H.', 'americanus .', 'It may grow to a length of 60 cm ( 24 in ) and a mass of 6 kilograms ( 13 lb ) , and bears a conspicuous pair of claws .', 'In life , the lobsters are blue , only becoming \" lobster red \" on cooking .']\n"
     ]
    }
   ],
   "source": [
    "def load_and_preprocess_wikitext(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Use regular expressions to split the text into sentences\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    sentences = [sentence.strip() for sentence in sentences]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "train_file_path = \"wikitext-2/wiki.train.tokens\"\n",
    "test_file_path = \"wikitext-2/wiki.test.tokens\"\n",
    "val_file_path = \"wikitext-2/wiki.valid.tokens\"\n",
    "\n",
    "wikitext_sentences_train = load_and_preprocess_wikitext(train_file_path)\n",
    "wikitext_sentences_test = load_and_preprocess_wikitext(test_file_path)\n",
    "wikitext_sentences_val = load_and_preprocess_wikitext(val_file_path)\n",
    "\n",
    "# Print the first few sentences to check\n",
    "print(\"\\nSample of train sentences:\")\n",
    "print(wikitext_sentences_train[:5])\n",
    "print(\"\\nSample of test sentences:\")\n",
    "print(wikitext_sentences_test[:5])\n",
    "print(\"\\nSample of val sentences:\")\n",
    "print(wikitext_sentences_val[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 33281\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "# Tokenize the data\n",
    "tokens = [word for sentence in wikitext_sentences_train for word in sentence.split()]\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = [PAD_TOKEN, UNK_TOKEN] + list(set(tokens))\n",
    "word_index = {word: index for index, word in enumerate(vocab)}\n",
    "# 添加新的tokens\n",
    "SOS_TOKEN = '<SOS>'\n",
    "EOS_TOKEN = '<EOS>'\n",
    "word_index[SOS_TOKEN] = len(word_index)\n",
    "word_index[EOS_TOKEN] = len(word_index)\n",
    "vocab = {v: k for k, v in word_index.items()}\n",
    "# Convert tokens to integers\n",
    "def tokenize_and_encode(text):\n",
    "    return [word_index.get(word, word_index[UNK_TOKEN]) for word in text.split()]\n",
    "\n",
    "encoded_data_train = [tokenize_and_encode(sentence) for sentence in wikitext_sentences_train]\n",
    "\n",
    "# Create a PyTorch Dataset\n",
    "class WikiDataset(Dataset):\n",
    "    def __init__(self, data, sequence_length):\n",
    "        self.data = data\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        if len(sample) < self.sequence_length:\n",
    "            sample.extend([word_index[PAD_TOKEN]] * (self.sequence_length - len(sample)))\n",
    "        else:\n",
    "            sample = sample[:self.sequence_length]\n",
    "        return torch.tensor(sample)\n",
    "\n",
    "# dataset = WikiDataset(encoded_data_train, SEQUENCE_LENGTH)\n",
    "# dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# Split the data into train and validation sets\n",
    "dataset = WikiDataset(encoded_data_train, SEQ_LEN)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Display a sample batch\n",
    "next(iter(train_dataloader))\n",
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "print(f'Vocabulary size: {VOCAB_SIZE}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j9/r2v28fnx719_1zz4f5yk8gbh0000gn/T/ipykernel_30901/800598550.py:92: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  gr.inputs.Slider(minimum=1, maximum=10, step=1, default=3, label=\"NUM_SENDERS\"),\n",
      "/var/folders/j9/r2v28fnx719_1zz4f5yk8gbh0000gn/T/ipykernel_30901/800598550.py:92: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  gr.inputs.Slider(minimum=1, maximum=10, step=1, default=3, label=\"NUM_SENDERS\"),\n",
      "/var/folders/j9/r2v28fnx719_1zz4f5yk8gbh0000gn/T/ipykernel_30901/800598550.py:93: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  gr.inputs.Slider(minimum=1, maximum=10, step=1, default=3, label=\"NUM_RECEIVERS\"),\n",
      "/var/folders/j9/r2v28fnx719_1zz4f5yk8gbh0000gn/T/ipykernel_30901/800598550.py:93: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  gr.inputs.Slider(minimum=1, maximum=10, step=1, default=3, label=\"NUM_RECEIVERS\"),\n",
      "/var/folders/j9/r2v28fnx719_1zz4f5yk8gbh0000gn/T/ipykernel_30901/800598550.py:94: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  gr.inputs.Slider(minimum=1000, maximum=20000, step=1000, default=10000, label=\"num_rounds\"),\n",
      "/var/folders/j9/r2v28fnx719_1zz4f5yk8gbh0000gn/T/ipykernel_30901/800598550.py:94: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  gr.inputs.Slider(minimum=1000, maximum=20000, step=1000, default=10000, label=\"num_rounds\"),\n",
      "/var/folders/j9/r2v28fnx719_1zz4f5yk8gbh0000gn/T/ipykernel_30901/800598550.py:97: GradioDeprecationWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  gr.outputs.Image(type=\"filepath\", label=\"Loss Curve\"),\n",
      "/var/folders/j9/r2v28fnx719_1zz4f5yk8gbh0000gn/T/ipykernel_30901/800598550.py:98: GradioDeprecationWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  gr.outputs.Textbox(label=\"Conversations\")\n",
      "/Users/YUAN/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/gradio/blocks.py:277: UserWarning: api_name predict already exists, using predict_1\n",
      "  warnings.warn(f\"api_name {api_name} already exists, using {api_name_}\")\n",
      "/Users/YUAN/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/gradio/blocks.py:277: UserWarning: api_name predict already exists, using predict_2\n",
      "  warnings.warn(f\"api_name {api_name} already exists, using {api_name_}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while flagging: field larger than field limit (131072)\n",
      "Error while flagging: field larger than field limit (131072)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import gradio as gr\n",
    "\n",
    "class MultiMultiSignalingGame:\n",
    "    def __init__(self, senders: list, receivers: list, optimizer, criterion):\n",
    "        self.senders = senders\n",
    "        self.receivers = receivers\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def play_round(self, states):\n",
    "        all_decoded_outputs = []\n",
    "        all_logits = []\n",
    "        \n",
    "        for i, sender in enumerate(self.senders):\n",
    "            # Sender encodes the state\n",
    "            logits, emb = sender(states[i])\n",
    "            all_logits.append(logits)\n",
    "            z = F.gumbel_softmax(logits, tau=TAU, hard=False, dim=-1)\n",
    "            \n",
    "            # Each receiver decodes the signal from the sender\n",
    "            for receiver in self.receivers:\n",
    "                decoded_output = receiver(emb, z)\n",
    "                all_decoded_outputs.append(decoded_output)\n",
    "      \n",
    "        # Calculate loss\n",
    "        loss = self.compute_loss(states, all_decoded_outputs, all_logits, beta=1.0)\n",
    "        \n",
    "        # Update model parameters\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Capture the input and output sentences\n",
    "        _, input_sentence_ids = torch.max(states[0], dim=1)\n",
    "        input_sentence_ids = input_sentence_ids.cpu().numpy()\n",
    "        input_sentence = ' '.join([vocab[idx] for idx in input_sentence_ids])\n",
    "\n",
    "        _, output_sentence_ids = torch.max(all_decoded_outputs[0][0], dim=1)\n",
    "        output_sentence_ids = output_sentence_ids.cpu().numpy()\n",
    "        output_sentence = ' '.join([vocab[idx] for idx in output_sentence_ids])\n",
    "\n",
    "        return loss.item(), input_sentence, output_sentence\n",
    "\n",
    "    def compute_loss(self, original_states, decoded_states, logits, beta):\n",
    "        recon_loss = sum([self.criterion(decoded_state.view(-1, VOCAB_SIZE), original_state.view(-1))\n",
    "                          for original_state, decoded_state in zip(original_states * len(self.receivers), decoded_states)])\n",
    "        \n",
    "        # Calculate KLD loss\n",
    "        kld_losses = []\n",
    "        for logit in logits:\n",
    "            mean, logvar = torch.chunk(logit, 2, dim=-1)\n",
    "            kld_loss = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "            kld_losses.append(kld_loss)\n",
    "\n",
    "        return recon_loss + beta * sum(kld_losses)\n",
    "\n",
    "def train_signal_game(NUM_SENDERS=3, NUM_RECEIVERS=3, num_rounds=10000):\n",
    "    senders = [TransformerEncoder().to(device) for _ in range(NUM_SENDERS)]\n",
    "    receivers = [TransformerDecoder().to(device) for _ in range(NUM_RECEIVERS)]\n",
    "    params = [list(sender.parameters()) for sender in senders]\n",
    "    params.extend([list(receiver.parameters()) for receiver in receivers])\n",
    "    optimizer = torch.optim.Adam([param for sublist in params for param in sublist], lr=0.001)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    game = MultiMultiSignalingGame(senders, receivers, optimizer, criterion)\n",
    "\n",
    "    losses = []\n",
    "    conversations = []\n",
    "    for round in range(num_rounds):\n",
    "        states = [torch.randint(VOCAB_SIZE, (BATCH_SIZE, 16)).to(device) for _ in range(NUM_SENDERS)]\n",
    "        loss, input_sentence, output_sentence = game.play_round(states)\n",
    "        losses.append(loss)\n",
    "        conversations.append(f\"Round {round+1} - Input: {input_sentence} | Output: {output_sentence}\")\n",
    "\n",
    "    conversation_str = \"\\n\".join(conversations)\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(losses, label='losses')\n",
    "    plt.xlabel('Round')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss Curve')\n",
    "    plt.savefig('loss_curve.png')\n",
    "    plt.close()\n",
    "\n",
    "    return 'loss_curve.png', conversation_str\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=train_signal_game,\n",
    "    inputs=[\n",
    "        gr.inputs.Slider(minimum=1, maximum=10, step=1, default=3, label=\"NUM_SENDERS\"),\n",
    "        gr.inputs.Slider(minimum=1, maximum=10, step=1, default=3, label=\"NUM_RECEIVERS\"),\n",
    "        gr.inputs.Slider(minimum=1000, maximum=20000, step=1000, default=10000, label=\"num_rounds\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.outputs.Image(type=\"filepath\", label=\"Loss Curve\"),\n",
    "        gr.outputs.Textbox(label=\"Conversations\")\n",
    "    ],\n",
    "    live=True\n",
    ")\n",
    "\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "st.session_state has no attribute \"attempts\". Did you forget to initialize it? More info: https://docs.streamlit.io/library/advanced-features/session-state#initialization",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/streamlit/runtime/state/session_state.py:378\u001b[0m, in \u001b[0;36mSessionState.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem(widget_id, key)\n\u001b[1;32m    379\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/streamlit/runtime/state/session_state.py:423\u001b[0m, in \u001b[0;36mSessionState._getitem\u001b[0;34m(self, widget_id, user_key)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39m# We'll never get here\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/streamlit/runtime/state/session_state_proxy.py:119\u001b[0m, in \u001b[0;36mSessionStateProxy.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m[key]\n\u001b[1;32m    120\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/streamlit/runtime/state/session_state_proxy.py:90\u001b[0m, in \u001b[0;36mSessionStateProxy.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     89\u001b[0m require_valid_user_key(key)\n\u001b[0;32m---> 90\u001b[0m \u001b[39mreturn\u001b[39;00m get_session_state()[key]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/streamlit/runtime/state/safe_session_state.py:113\u001b[0m, in \u001b[0;36mSafeSessionState.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n\u001b[0;32m--> 113\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_state[key]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/streamlit/runtime/state/session_state.py:380\u001b[0m, in \u001b[0;36mSessionState.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m--> 380\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(_missing_key_error_message(key))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'st.session_state has no key \"attempts\". Did you forget to initialize it? More info: https://docs.streamlit.io/library/advanced-features/session-state#initialization'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 101\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     attempts \u001b[39m=\u001b[39m st\u001b[39m.\u001b[39;49msession_state\u001b[39m.\u001b[39;49mattempts\n\u001b[1;32m    102\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/streamlit/runtime/state/session_state_proxy.py:121\u001b[0m, in \u001b[0;36mSessionStateProxy.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m--> 121\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(_missing_attr_error_message(key))\n",
      "\u001b[0;31mAttributeError\u001b[0m: st.session_state has no attribute \"attempts\". Did you forget to initialize it? More info: https://docs.streamlit.io/library/advanced-features/session-state#initialization",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/streamlit/runtime/state/session_state.py:378\u001b[0m, in \u001b[0;36mSessionState.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem(widget_id, key)\n\u001b[1;32m    379\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/streamlit/runtime/state/session_state.py:423\u001b[0m, in \u001b[0;36mSessionState._getitem\u001b[0;34m(self, widget_id, user_key)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39m# We'll never get here\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/streamlit/runtime/state/session_state_proxy.py:119\u001b[0m, in \u001b[0;36mSessionStateProxy.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m[key]\n\u001b[1;32m    120\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/streamlit/runtime/state/session_state_proxy.py:90\u001b[0m, in \u001b[0;36mSessionStateProxy.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     89\u001b[0m require_valid_user_key(key)\n\u001b[0;32m---> 90\u001b[0m \u001b[39mreturn\u001b[39;00m get_session_state()[key]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/streamlit/runtime/state/safe_session_state.py:113\u001b[0m, in \u001b[0;36mSafeSessionState.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n\u001b[0;32m--> 113\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_state[key]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/streamlit/runtime/state/session_state.py:380\u001b[0m, in \u001b[0;36mSessionState.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m--> 380\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(_missing_key_error_message(key))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'st.session_state has no key \"attempts\". Did you forget to initialize it? More info: https://docs.streamlit.io/library/advanced-features/session-state#initialization'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 124\u001b[0m\n\u001b[1;32m    121\u001b[0m         attempts\u001b[39m.\u001b[39mappend(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSenders: \u001b[39m\u001b[39m{\u001b[39;00mNUM_SENDERS\u001b[39m}\u001b[39;00m\u001b[39m, Receivers: \u001b[39m\u001b[39m{\u001b[39;00mNUM_RECEIVERS\u001b[39m}\u001b[39;00m\u001b[39m, Rounds: \u001b[39m\u001b[39m{\u001b[39;00mnum_rounds\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    123\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 124\u001b[0m     main()\n\u001b[1;32m    125\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mstreamlit run /Users/YUAN/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/ipykernel_launcher.py\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 104\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     st\u001b[39m.\u001b[39msession_state\u001b[39m.\u001b[39mattempts \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 104\u001b[0m     attempts \u001b[39m=\u001b[39m st\u001b[39m.\u001b[39;49msession_state\u001b[39m.\u001b[39;49mattempts\n\u001b[1;32m    106\u001b[0m \u001b[39m# 显示过往所有尝试的参数组合\u001b[39;00m\n\u001b[1;32m    107\u001b[0m st\u001b[39m.\u001b[39msidebar\u001b[39m.\u001b[39mtext(\u001b[39m\"\u001b[39m\u001b[39mPrevious Attempts:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/streamlit/runtime/state/session_state_proxy.py:121\u001b[0m, in \u001b[0;36mSessionStateProxy.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[key]\n\u001b[1;32m    120\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m--> 121\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(_missing_attr_error_message(key))\n",
      "\u001b[0;31mAttributeError\u001b[0m: st.session_state has no attribute \"attempts\". Did you forget to initialize it? More info: https://docs.streamlit.io/library/advanced-features/session-state#initialization"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import gradio as gr\n",
    "\n",
    "class MultiMultiSignalingGame:\n",
    "    def __init__(self, senders: list, receivers: list, optimizer, criterion):\n",
    "        self.senders = senders\n",
    "        self.receivers = receivers\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def play_round(self, states):\n",
    "        all_decoded_outputs = []\n",
    "        all_logits = []\n",
    "        \n",
    "        for i, sender in enumerate(self.senders):\n",
    "            # Sender encodes the state\n",
    "            logits, emb = sender(states[i])\n",
    "            all_logits.append(logits)\n",
    "            z = F.gumbel_softmax(logits, tau=TAU, hard=False, dim=-1)\n",
    "            \n",
    "            # Each receiver decodes the signal from the sender\n",
    "            for receiver in self.receivers:\n",
    "                decoded_output = receiver(emb, z)\n",
    "                all_decoded_outputs.append(decoded_output)\n",
    "      \n",
    "        # Calculate loss\n",
    "        loss = self.compute_loss(states, all_decoded_outputs, all_logits, beta=1.0)\n",
    "        \n",
    "        # Update model parameters\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Capture the input and output sentences\n",
    "        _, input_sentence_ids = torch.max(states[0], dim=1)\n",
    "        input_sentence_ids = input_sentence_ids.cpu().numpy()\n",
    "        input_sentence = ' '.join([vocab[idx] for idx in input_sentence_ids])\n",
    "\n",
    "        _, output_sentence_ids = torch.max(all_decoded_outputs[0][0], dim=1)\n",
    "        output_sentence_ids = output_sentence_ids.cpu().numpy()\n",
    "        output_sentence = ' '.join([vocab[idx] for idx in output_sentence_ids])\n",
    "\n",
    "        return loss.item(), input_sentence, output_sentence\n",
    "\n",
    "    def compute_loss(self, original_states, decoded_states, logits, beta):\n",
    "        recon_loss = sum([self.criterion(decoded_state.view(-1, VOCAB_SIZE), original_state.view(-1))\n",
    "                          for original_state, decoded_state in zip(original_states * len(self.receivers), decoded_states)])\n",
    "        \n",
    "        # Calculate KLD loss\n",
    "        kld_losses = []\n",
    "        for logit in logits:\n",
    "            mean, logvar = torch.chunk(logit, 2, dim=-1)\n",
    "            kld_loss = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "            kld_losses.append(kld_loss)\n",
    "\n",
    "        return recon_loss + beta * sum(kld_losses)\n",
    "\n",
    "def train_signal_game(NUM_SENDERS=3, NUM_RECEIVERS=3, num_rounds=10000):\n",
    "    senders = [TransformerEncoder().to(device) for _ in range(NUM_SENDERS)]\n",
    "    receivers = [TransformerDecoder().to(device) for _ in range(NUM_RECEIVERS)]\n",
    "    params = [list(sender.parameters()) for sender in senders]\n",
    "    params.extend([list(receiver.parameters()) for receiver in receivers])\n",
    "    optimizer = torch.optim.Adam([param for sublist in params for param in sublist], lr=0.001)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    game = MultiMultiSignalingGame(senders, receivers, optimizer, criterion)\n",
    "\n",
    "    losses = []\n",
    "    conversations = []\n",
    "    for round in range(num_rounds):\n",
    "        states = [torch.randint(VOCAB_SIZE, (BATCH_SIZE, 16)).to(device) for _ in range(NUM_SENDERS)]\n",
    "        loss, input_sentence, output_sentence = game.play_round(states)\n",
    "        losses.append(loss)\n",
    "        conversations.append(f\"Round {round+1} - Input: {input_sentence} | Output: {output_sentence}\")\n",
    "\n",
    "    conversation_str = \"\\n\".join(conversations)\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(losses, label='losses')\n",
    "    plt.xlabel('Round')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss Curve')\n",
    "    plt.savefig('loss_curve.png')\n",
    "    plt.close()\n",
    "\n",
    "    return 'loss_curve.png', conversation_str\n",
    "\n",
    "def main():\n",
    "    st.title('Transformer-based CatVAE and Signal Game')\n",
    "\n",
    "    # 添加 sliders 来选择参数\n",
    "    NUM_SENDERS = st.sidebar.slider('NUM_SENDERS', 1, 10, 3)\n",
    "    NUM_RECEIVERS = st.sidebar.slider('NUM_RECEIVERS', 1, 10, 3)\n",
    "    num_rounds = st.sidebar.slider('num_rounds', 1000, 20000, 10000)\n",
    "\n",
    "    # 为了记录所有的参数组合尝试\n",
    "    try:\n",
    "        attempts = st.session_state.attempts\n",
    "    except AttributeError:\n",
    "        st.session_state.attempts = []\n",
    "        attempts = st.session_state.attempts\n",
    "\n",
    "    # 显示过往所有尝试的参数组合\n",
    "    st.sidebar.text(\"Previous Attempts:\")\n",
    "    for attempt in attempts:\n",
    "        st.sidebar.text(attempt)\n",
    "\n",
    "    # 添加一个按钮来开始游戏\n",
    "    if st.button('Start'):\n",
    "        # 运行游戏并获取结果\n",
    "        losses, conversations = play_game(NUM_SENDERS, NUM_RECEIVERS, num_rounds)  # 假设 play_game 是实际执行信号游戏的函数\n",
    "\n",
    "        # 显示结果\n",
    "        st.line_chart(losses, use_container_width=True)\n",
    "        st.text_area(\"Conversations\", \"\\n\".join(conversations), height=200)\n",
    "\n",
    "        # 保存这次尝试的参数组合\n",
    "        attempts.append(f\"Senders: {NUM_SENDERS}, Receivers: {NUM_RECEIVERS}, Rounds: {num_rounds}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "!streamlit run /Users/YUAN/opt/anaconda3/envs/myCVAE/lib/python3.11/site-packages/ipykernel_launcher.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myCVAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
