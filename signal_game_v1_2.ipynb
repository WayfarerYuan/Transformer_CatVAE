{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Model Setup & Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-08-17T14:18:54.087366Z",
     "iopub.status.busy": "2023-08-17T14:18:54.087233Z",
     "iopub.status.idle": "2023-08-17T14:18:54.996228Z",
     "shell.execute_reply": "2023-08-17T14:18:54.995600Z",
     "shell.execute_reply.started": "2023-08-17T14:18:54.087349Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import random\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-08-17T14:18:54.997403Z",
     "iopub.status.busy": "2023-08-17T14:18:54.997260Z",
     "iopub.status.idle": "2023-08-17T14:18:55.022877Z",
     "shell.execute_reply": "2023-08-17T14:18:55.022376Z",
     "shell.execute_reply.started": "2023-08-17T14:18:54.997386Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4d3b694f60>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define constants\n",
    "EMBEDDING_DIM = 16\n",
    "HIDDEN_DIM = 16\n",
    "LATENT_DIM = 16 # Dimension of the latent space\n",
    "SEQ_LEN = 16 # Max length of the sequence\n",
    "NHEAD = 2\n",
    "NUM_LAYERS = 2\n",
    "\n",
    "# Gumbel softmax temperature\n",
    "TAU = 1.0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.random.manual_seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2023-08-17T14:18:55.024256Z",
     "iopub.status.busy": "2023-08-17T14:18:55.024116Z",
     "iopub.status.idle": "2023-08-17T14:18:55.065931Z",
     "shell.execute_reply": "2023-08-17T14:18:55.065380Z",
     "shell.execute_reply.started": "2023-08-17T14:18:55.024238Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pass embeded into decoder instead of using the original x\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model=EMBEDDING_DIM, nhead=NHEAD, num_layers=NUM_LAYERS):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, d_model)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            # multi-head attention, fead-forward net\n",
    "            nn.TransformerEncoderLayer(d_model, nhead), num_layers \n",
    "        )\n",
    "        self.fc_logits = nn.Linear(d_model, LATENT_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x).permute(1, 0, 2)  # Transformer expects seq_len, batch, features\n",
    "        transformed = self.transformer_encoder(embedded)\n",
    "        # Use the final state to predict logits for latent space\n",
    "        logits = self.fc_logits(transformed[-1])\n",
    "        return logits, embedded\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model=EMBEDDING_DIM, nhead=NHEAD, num_layers=NUM_LAYERS):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, d_model)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model, nhead), num_layers\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, VOCAB_SIZE)\n",
    "        self.fc_z = nn.Linear(LATENT_DIM, d_model)  # Convert z to feature size for transformer\n",
    "\n",
    "    def forward(self, embedded, z):\n",
    "        # embedded = self.embedding(x).permute(1, 0, 2) # Transformer expects [seq_len, batch, features], permute函数用于改变张量的维度顺序\n",
    "        z_adjusted = self.fc_z(z).unsqueeze(0)\n",
    "        output = self.transformer_decoder(embedded, z_adjusted)\n",
    "        return self.fc_out(output.permute(1, 0, 2))\n",
    "\n",
    "\n",
    "class TransformerCVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerCVAE, self).__init__()\n",
    "        self.encoder = TransformerEncoder()\n",
    "        self.decoder = TransformerDecoder()\n",
    "\n",
    "    def reparameterize(self, logits):\n",
    "        return F.gumbel_softmax(logits, tau=TAU, hard=False, dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits, emb = self.encoder(x)\n",
    "        z = self.reparameterize(logits)\n",
    "        return self.decoder(emb, z), logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-17T14:18:55.067255Z",
     "iopub.status.busy": "2023-08-17T14:18:55.067111Z",
     "iopub.status.idle": "2023-08-17T14:18:55.346643Z",
     "shell.execute_reply": "2023-08-17T14:18:55.346069Z",
     "shell.execute_reply.started": "2023-08-17T14:18:55.067237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of train sentences:\n",
      "['= Valkyria Chronicles III = \\n \\n Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit .', 'Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable .', 'Released in January 2011 in Japan , it is the third game in the Valkyria series .', '<unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" .', 'The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II .']\n",
      "\n",
      "Sample of test sentences:\n",
      "['= Robert <unk> = \\n \\n Robert <unk> is an English film , television and theatre actor .', 'He had a guest @-@ starring role on the television series The Bill in 2000 .', 'This was followed by a starring role in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre .', 'He had a guest role in the television series Judge John <unk> in 2002 .', 'In 2004 <unk> landed a role as \" Craig \" in the episode \" Teddy \\'s Story \" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi .']\n",
      "\n",
      "Sample of val sentences:\n",
      "['= Homarus gammarus = \\n \\n Homarus gammarus , known as the European lobster or common lobster , is a species of <unk> lobster from the eastern Atlantic Ocean , Mediterranean Sea and parts of the Black Sea .', 'It is closely related to the American lobster , H.', 'americanus .', 'It may grow to a length of 60 cm ( 24 in ) and a mass of 6 kilograms ( 13 lb ) , and bears a conspicuous pair of claws .', 'In life , the lobsters are blue , only becoming \" lobster red \" on cooking .']\n"
     ]
    }
   ],
   "source": [
    "def load_and_preprocess_wikitext(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Use regular expressions to split the text into sentences\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    sentences = [sentence.strip() for sentence in sentences]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "train_file_path = \"wikitext-2/wiki.train.tokens\"\n",
    "test_file_path = \"wikitext-2/wiki.test.tokens\"\n",
    "val_file_path = \"wikitext-2/wiki.valid.tokens\"\n",
    "\n",
    "wikitext_sentences_train = load_and_preprocess_wikitext(train_file_path)\n",
    "wikitext_sentences_test = load_and_preprocess_wikitext(test_file_path)\n",
    "wikitext_sentences_val = load_and_preprocess_wikitext(val_file_path)\n",
    "\n",
    "# Print the first few sentences to check\n",
    "print(\"\\nSample of train sentences:\")\n",
    "print(wikitext_sentences_train[:5])\n",
    "print(\"\\nSample of test sentences:\")\n",
    "print(wikitext_sentences_test[:5])\n",
    "print(\"\\nSample of val sentences:\")\n",
    "print(wikitext_sentences_val[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-08-17T14:18:55.347706Z",
     "iopub.status.busy": "2023-08-17T14:18:55.347557Z",
     "iopub.status.idle": "2023-08-17T14:18:56.322361Z",
     "shell.execute_reply": "2023-08-17T14:18:56.321540Z",
     "shell.execute_reply.started": "2023-08-17T14:18:55.347687Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8654, 29601,  2261,   525, 29601, 13930, 18511, 12429, 15329, 24512,\n",
      "         25406,  8775, 18511, 31183,  5352, 32887],\n",
      "        [23039, 11951, 11953, 15329, 29601, 13596,   525, 26583, 28942, 29601,\n",
      "         12212, 30327,   525,  8304, 12556, 15050],\n",
      "        [17267,  7254, 30312, 29601, 26177,  2045, 18511, 28340,  6608, 11229,\n",
      "          9651, 11229, 18511, 24330, 29208,     0],\n",
      "        [11943, 11229, 16139,  5382, 10678, 29601, 13412, 27904,   525,  6837,\n",
      "         15329, 29601, 21665,   525, 29601,  2975],\n",
      "        [29181,  1854, 22698,  6608, 24769,   864, 21078, 20748, 10268, 31966,\n",
      "         25805, 27264, 28447, 28998, 27199,  8571],\n",
      "        [27851, 32862, 27305, 25406, 27026, 11816, 27199, 12308, 27199, 15329,\n",
      "          5588, 28447,   525, 28447, 13462, 27026],\n",
      "        [31757, 15329, 29601, 29196, 16206,  1132, 28012,   714, 17822, 31468,\n",
      "           525, 17615, 27334,  8478, 27026, 17615],\n",
      "        [16760, 29601, 27454,   525, 28420, 10290, 10039,  7357, 29932,  6608,\n",
      "         18148, 31190, 28447, 23958, 17448, 29912],\n",
      "        [17699, 30836, 25406, 33270, 29912,  4693, 13194, 22899,  2778, 27026,\n",
      "          4693, 13194, 22899, 12990, 29208,     0],\n",
      "        [19431, 21536, 29601,  9470, 10698, 20832,   525, 29601,  3373, 11229,\n",
      "          9651, 15329, 10511, 21933,  8766,  5127],\n",
      "        [17699,  3071,  6174,  6523, 28447, 24140, 19389,  6793, 14422, 15329,\n",
      "         17409, 24538,  8797, 23195, 20475, 27026],\n",
      "        [ 4668,   323, 17699, 28149, 26161, 25406, 18511, 26177,  6408,  9750,\n",
      "         15329, 27136, 18597, 29208,     0,     0],\n",
      "        [29269, 19620, 24620, 30164, 30224, 32965, 20414,  5106, 18511, 29180,\n",
      "         26389, 12936, 32126,  7190, 18604, 27277],\n",
      "        [ 8508, 29601,  2892, 20058, 11790, 15329, 24359,  7190, 31611,   706,\n",
      "         18511, 22468, 21762, 22381, 17715, 12240],\n",
      "        [20356, 27026, 15922, 27199,   525, 12902, 29550,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [19877, 19771, 28754, 17409, 27199, 15329, 13964,  6608, 18511, 29249,\n",
      "         15329, 13786, 13498, 21475, 29601, 18013]])\n",
      "Vocabulary size: 33281\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "# Tokenize the data\n",
    "tokens = [word for sentence in wikitext_sentences_train for word in sentence.split()]\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = [PAD_TOKEN, UNK_TOKEN] + list(set(tokens))\n",
    "word_index = {word: index for index, word in enumerate(vocab)}\n",
    "# 添加新的tokens\n",
    "SOS_TOKEN = '<SOS>'\n",
    "EOS_TOKEN = '<EOS>'\n",
    "word_index[SOS_TOKEN] = len(word_index)\n",
    "word_index[EOS_TOKEN] = len(word_index)\n",
    "vocab = {v: k for k, v in word_index.items()}\n",
    "# Convert tokens to integers\n",
    "def tokenize_and_encode(text):\n",
    "    return [word_index.get(word, word_index[UNK_TOKEN]) for word in text.split()]\n",
    "\n",
    "encoded_data_train = [tokenize_and_encode(sentence) for sentence in wikitext_sentences_train]\n",
    "\n",
    "# Create a PyTorch Dataset\n",
    "class WikiDataset(Dataset):\n",
    "    def __init__(self, data, sequence_length):\n",
    "        self.data = data\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        if len(sample) < self.sequence_length:\n",
    "            sample.extend([word_index[PAD_TOKEN]] * (self.sequence_length - len(sample)))\n",
    "        else:\n",
    "            sample = sample[:self.sequence_length]\n",
    "        return torch.tensor(sample)\n",
    "\n",
    "# dataset = WikiDataset(encoded_data_train, SEQUENCE_LENGTH)\n",
    "# dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# Split the data into train and validation sets\n",
    "dataset = WikiDataset(encoded_data_train, SEQ_LEN)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Display a sample batch\n",
    "print(next(iter(train_dataloader)))\n",
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "print(f'Vocabulary size: {VOCAB_SIZE}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-08-17T14:18:56.323582Z",
     "iopub.status.busy": "2023-08-17T14:18:56.323424Z",
     "iopub.status.idle": "2023-08-17T14:18:58.927508Z",
     "shell.execute_reply": "2023-08-17T14:18:58.926957Z",
     "shell.execute_reply.started": "2023-08-17T14:18:56.323563Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initializing the model with the set hyperparameters\n",
    "transformer_cvae = TransformerCVAE()\n",
    "transformer_cvae.to(device)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(transformer_cvae.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-08-17T14:18:58.928615Z",
     "iopub.status.busy": "2023-08-17T14:18:58.928462Z",
     "iopub.status.idle": "2023-08-17T14:21:50.324754Z",
     "shell.execute_reply": "2023-08-17T14:21:50.324217Z",
     "shell.execute_reply.started": "2023-08-17T14:18:58.928597Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 | Loss: 2.1682243 = Recon: 2.1681833 + KLD: 0.0000410:  33%|███▎      | 3928/11784 [00:56<01:51, 70.52it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: There were several variants of the <unk> design . ---> Echo: There were several upon of the <unk> design .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 | Loss: 0.1416055 = Recon: 0.1416039 + KLD: 0.0000016:  67%|██████▋   | 7856/11784 [01:52<00:52, 74.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: There were several variants of the <unk> design . ---> Echo: There were several variants of the <unk> design .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 | Loss: 0.0357896 = Recon: 0.0357886 + KLD: 0.0000010: 100%|██████████| 11784/11784 [02:48<00:00, 73.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: There were several variants of the <unk> design . ---> Echo: There were several variants of the <unk> design .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 | Loss: 0.0357896 = Recon: 0.0357886 + KLD: 0.0000010: 100%|██████████| 11784/11784 [02:51<00:00, 68.90it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_text(model, sentence, vocab):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Tokenize and encode the sentence\n",
    "        encoded_sentence = tokenize_and_encode(sentence)\n",
    "        input_tensor = torch.tensor([encoded_sentence], dtype=torch.long).to(device)  # Ensure type is Long\n",
    "        \n",
    "        # Pass the encoded sentence through the model encoder\n",
    "        with torch.no_grad():\n",
    "            logits, emb = model.encoder(input_tensor)\n",
    "            z = model.reparameterize(logits)\n",
    "            output = model.decoder(emb, z)\n",
    "        \n",
    "        # Convert the output probabilities to predicted token IDs\n",
    "        _, predicted_ids = torch.max(output, dim=2)\n",
    "        predicted_ids = predicted_ids.squeeze().cpu().numpy()\n",
    "        \n",
    "        # Convert the predicted token IDs back to words\n",
    "        predicted_sentence = ' '.join([vocab[idx] for idx in predicted_ids])\n",
    "        \n",
    "        return predicted_sentence\n",
    "\n",
    "def combined_loss_fn(recon_output, target, logits, beta=1.0):\n",
    "    # 计算重构损失\n",
    "    # recon_output = torch.clamp(recon_output, 1e-10, 1 - 1e-10)\n",
    "    recon_loss = F.cross_entropy(recon_output.view(-1, VOCAB_SIZE), target.view(-1), reduction='mean')\n",
    "\n",
    "    # 计算KLD损失\n",
    "    mean, logvar = torch.chunk(logits, 2, dim=-1)\n",
    "    kld_loss = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "\n",
    "    # 计算总损失\n",
    "    total_loss = recon_loss + beta * kld_loss\n",
    "\n",
    "    return total_loss, recon_loss, kld_loss\n",
    "\n",
    "\n",
    "valid_sentence = \"There were several variants of the <unk> design .\"\n",
    "\n",
    "def train_and_visualize(model, train_dataloader, val_dataloader, optimizer, num_epochs, word_index, vocab, beta=1.0):\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    recon_losses = []\n",
    "    kld_losses = []\n",
    "    total_steps = len(train_dataloader) * num_epochs\n",
    "\n",
    "    # Initialize tqdm progress bar\n",
    "    progress_bar = tqdm(total=total_steps, desc=\"Training\", position=0)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_dataloader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output, logits = model(batch)\n",
    "            \n",
    "            # 使用新的损失函数\n",
    "            loss, recon_loss, kld_loss = combined_loss_fn(output, batch, logits, beta)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            recon_losses.append(recon_loss.item())\n",
    "            kld_losses.append(kld_loss.item())\n",
    "            progress_bar.set_description(f'Epoch {epoch+1}/{num_epochs} | Loss: {loss.item():.7f} = Recon: {recon_loss.item():.7f} + KLD: {kld_loss.item():.7f}')\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        # Generate text after each epoch\n",
    "        generated_sentence = generate_text(model, valid_sentence, vocab)\n",
    "        print(f\"Input: {valid_sentence} ---> Echo: {generated_sentence}\\n\")\n",
    "        \n",
    "        # Print epoch loss\n",
    "        avg_loss = sum(losses[-len(train_dataloader):]) / len(train_dataloader)\n",
    "        # print(f\"Done Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_loss:.7f}\")\n",
    "\n",
    "        # Validation after each epoch\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            avg_val_loss = 0\n",
    "            for batch in val_dataloader:\n",
    "                batch = batch.to(device)\n",
    "                output, logits = model(batch)\n",
    "                val_loss, _, _ = combined_loss_fn(output, batch, logits, beta)\n",
    "                val_losses.append(val_loss.item())\n",
    "                avg_val_loss += val_loss.item()\n",
    "            avg_val_loss /= len(val_dataloader)\n",
    "            # print(f\"Done Epoch [{epoch+1}/{num_epochs}] - Validation Loss: {avg_val_loss:.7f}\")\n",
    "\n",
    "        \n",
    "\n",
    "    # Close the tqdm progress bar\n",
    "    progress_bar.close()\n",
    "    \n",
    "    torch.save(transformer_cvae.state_dict(), 'transformer_cvae.dict')\n",
    "\n",
    "    # Plot the loss curve\n",
    "    plt.plot(losses, label='Total Loss')\n",
    "    plt.plot(recon_losses, label='Reconstruction Loss')\n",
    "    plt.plot(kld_losses, label='KLD Loss')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the validation loss curve\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Batches')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Validation Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "num_epochs = 3\n",
    "train_and_visualize(transformer_cvae, train_dataloader, val_dataloader, optimizer, num_epochs, word_index, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-08-17T14:21:50.327063Z",
     "iopub.status.busy": "2023-08-17T14:21:50.326901Z",
     "iopub.status.idle": "2023-08-17T14:21:50.333552Z",
     "shell.execute_reply": "2023-08-17T14:21:50.333093Z",
     "shell.execute_reply.started": "2023-08-17T14:21:50.327045Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Robert <unk> is an English film , television and theatre actor .'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_sentence = \"Robert <unk> is an English film , television and theatre actor .\"\n",
    "test_generate = generate_text(transformer_cvae, valid_sentence, vocab)\n",
    "test_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-08-17T14:21:50.334957Z",
     "iopub.status.busy": "2023-08-17T14:21:50.334815Z",
     "iopub.status.idle": "2023-08-17T14:21:50.337514Z",
     "shell.execute_reply": "2023-08-17T14:21:50.336988Z",
     "shell.execute_reply.started": "2023-08-17T14:21:50.334939Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Visualize the latent space (t-SNE)\n",
    "# from sklearn.manifold import TSNE\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def visualize_latent_space(model, dataloader, device):\n",
    "#     model.eval()\n",
    "    \n",
    "#     # Collect logits from the encoder\n",
    "#     logits_list = []\n",
    "#     with torch.no_grad():\n",
    "#         for batch in dataloader:\n",
    "#             batch = batch.to(device)\n",
    "#             logits, _ = model.encoder(batch)\n",
    "#             logits_list.append(logits)\n",
    "    \n",
    "#     logits_array = torch.cat(logits_list).cpu().numpy()\n",
    "\n",
    "#     # Perform t-SNE\n",
    "#     tsne = TSNE(n_components=2)\n",
    "#     logits_2d = tsne.fit_transform(logits_array)\n",
    "\n",
    "#     # Plot\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     plt.scatter(logits_2d[:, 0], logits_2d[:, 1], alpha=0.5)\n",
    "#     plt.title(\"t-SNE visualization of logits\")\n",
    "#     plt.show()\n",
    "\n",
    "# # After training, call the visualization function\n",
    "# visualize_latent_space(transformer_cvae, train_dataloader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: 'Signal Game'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-08-17T14:21:50.338379Z",
     "iopub.status.busy": "2023-08-17T14:21:50.338249Z",
     "iopub.status.idle": "2023-08-17T14:23:47.628027Z",
     "shell.execute_reply": "2023-08-17T14:23:47.627462Z",
     "shell.execute_reply.started": "2023-08-17T14:21:50.338363Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Round[10000/10000] loss: 0.1224908: 100%|██████████| 10000/10000 [01:57<00:00, 85.35it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class SignalingGame:\n",
    "    def __init__(self, sender: TransformerEncoder, receiver: TransformerDecoder, optimizer, criterion):\n",
    "        self.sender = sender\n",
    "        self.receiver = receiver\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def play_round(self, state):\n",
    "        # # Convert the state to predicted token IDs\n",
    "        # predicted_ids_batch = state.cpu().numpy()\n",
    "        # for i in range(predicted_ids_batch.shape[0]):\n",
    "        #     predicted_ids = predicted_ids_batch[i]\n",
    "        #     # Convert the predicted token IDs back to words\n",
    "        #     state_sentence = ' '.join([vocab[idx] for idx in predicted_ids])\n",
    "        #     print(f'state_sentence_{i}:{state_sentence}')\n",
    "        \n",
    "        # 发信者编码状态\n",
    "        logits, emb = self.sender(state)\n",
    "        z = F.gumbel_softmax(logits, tau=TAU, hard=False, dim=-1)\n",
    "        \n",
    "        # 接收者解码信号\n",
    "        decoded_output = self.receiver(emb, z)\n",
    "      \n",
    "        # # Convert the output probabilities to predicted token IDs\n",
    "        # _, predicted_ids_batch = torch.max(decoded_output, dim=2)\n",
    "        # predicted_ids_batch = predicted_ids_batch.cpu().numpy()\n",
    "        # for i in range(predicted_ids_batch.shape[0]):\n",
    "        #     predicted_ids = predicted_ids_batch[i]\n",
    "        #     # Convert the predicted token IDs back to words\n",
    "        #     decoded_sentence = ' '.join([vocab[idx] for idx in predicted_ids])\n",
    "        #     print(f'decoded_sentence_{i}:{decoded_sentence}')\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = self.compute_loss(state, decoded_output, logits, beta = 10.0)\n",
    "        \n",
    "        # 更新模型参数\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def compute_loss(self, original_state, decoded_state, logits, beta):\n",
    "        recon_loss = self.criterion(decoded_state.view(-1, VOCAB_SIZE), original_state.view(-1))\n",
    "        # 计算KLD损失\n",
    "        mean, logvar = torch.chunk(logits, 2, dim=-1)\n",
    "        kld_loss = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "\n",
    "        return recon_loss + beta * kld_loss\n",
    "\n",
    "\n",
    "# 初始化发信者和接收者\n",
    "sender = TransformerEncoder().to(device)\n",
    "receiver = TransformerDecoder().to(device)\n",
    "\n",
    "# 为游戏定义优化器和损失函数\n",
    "optimizer = torch.optim.Adam(list(sender.parameters()) + list(receiver.parameters()), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "game = SignalingGame(sender, receiver, optimizer, criterion)\n",
    "\n",
    "# 模拟多轮信号游戏\n",
    "num_rounds = 10000\n",
    "losses = []\n",
    "progress_bar = tqdm(total=num_rounds, desc=\"Playing rounds\")\n",
    "for round in range(num_rounds):\n",
    "    state = torch.randint(VOCAB_SIZE, (BATCH_SIZE, 16)).to(device)  # 随机生成一个batch的16词长度的句子作为状态\n",
    "    loss = game.play_round(state)\n",
    "    losses.append(loss)\n",
    "    progress_bar.set_description(f' Round[{round+1}/{num_rounds}] loss: {loss:.7f}')\n",
    "    progress_bar.update()\n",
    "progress_bar.close()\n",
    "# 绘制损失曲线\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses, label='losses')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-08-17T14:23:47.629069Z",
     "iopub.status.busy": "2023-08-17T14:23:47.628905Z",
     "iopub.status.idle": "2023-08-17T14:23:47.632709Z",
     "shell.execute_reply": "2023-08-17T14:23:47.632173Z",
     "shell.execute_reply.started": "2023-08-17T14:23:47.629050Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class SignalingGameModified:\n",
    "#     def __init__(self, sender: TransformerEncoder, receiver: TransformerDecoder, optimizer, criterion):\n",
    "#         self.sender = sender\n",
    "#         self.receiver = receiver\n",
    "#         self.optimizer = optimizer\n",
    "#         self.criterion = criterion\n",
    "\n",
    "#     def play_round(self, state):\n",
    "#         # Sender encodes the state\n",
    "#         logits, emb = self.sender(state)\n",
    "#         z = F.gumbel_softmax(logits, tau=TAU, hard=False, dim=-1)\n",
    "        \n",
    "#         # Receiver decodes the signal from the sender\n",
    "#         decoded_output = self.receiver(emb, z)\n",
    "      \n",
    "#         # Calculate loss\n",
    "#         total_loss, recon_loss, kld_loss = self.compute_loss(state, decoded_output, logits, beta=10.0)\n",
    "        \n",
    "#         # Update model parameters\n",
    "#         self.optimizer.zero_grad()\n",
    "#         total_loss.backward()\n",
    "#         self.optimizer.step()\n",
    "\n",
    "#         return total_loss.item(), recon_loss.item(), kld_loss.item()\n",
    "\n",
    "#     def compute_loss(self, original_state, decoded_state, logits, beta):\n",
    "#         recon_loss = self.criterion(decoded_state.view(-1, VOCAB_SIZE), original_state.view(-1))\n",
    "#         # Calculate KLD loss\n",
    "#         mean, logvar = torch.chunk(logits, 2, dim=-1)\n",
    "#         kld_loss = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "\n",
    "#         return recon_loss + beta * kld_loss, recon_loss, kld_loss\n",
    "\n",
    "# # Initialize senders and receivers\n",
    "# sender = TransformerEncoder().to(device)\n",
    "# receiver = TransformerDecoder().to(device)\n",
    "\n",
    "# # Define optimizer and loss function for the game\n",
    "# optimizer = torch.optim.Adam(list(sender.parameters()) + list(receiver.parameters()), lr=0.001)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# game = SignalingGameModified(sender, receiver, optimizer, criterion)\n",
    "\n",
    "# # Simulate multiple rounds of the signaling game\n",
    "# num_rounds = 10000\n",
    "# total_losses = []\n",
    "# recon_losses = []\n",
    "# kld_losses = []\n",
    "# recorded_sentences = []\n",
    "\n",
    "# progress_bar = tqdm(total=num_rounds, desc=\"Playing rounds\")\n",
    "# for round in range(num_rounds):\n",
    "#     state = torch.randint(VOCAB_SIZE, (BATCH_SIZE, 16)).to(device)  # Randomly generate a batch of 16-word sentences as state\n",
    "#     total_loss, recon_loss, kld_loss = game.play_round(state)\n",
    "    \n",
    "#     total_losses.append(total_loss)\n",
    "#     recon_losses.append(recon_loss)\n",
    "#     kld_losses.append(kld_loss)\n",
    "    \n",
    "#     # Record sentences every 1/10 rounds\n",
    "#     if round % (num_rounds // 10) == 0:\n",
    "#         predicted_ids_batch = state.cpu().numpy()\n",
    "#         for i in range(predicted_ids_batch.shape[0]):\n",
    "#             predicted_ids = predicted_ids_batch[i]\n",
    "#             sentence = ' '.join([vocab[idx] for idx in predicted_ids])\n",
    "#             recorded_sentences.append((round, i, sentence))\n",
    "    \n",
    "#     progress_bar.set_description(f' Round[{round+1}/{num_rounds}] Total Loss: {total_loss:.7f}, Recon Loss: {recon_loss:.7f}, KLD Loss: {kld_loss:.7f}')\n",
    "#     progress_bar.update()\n",
    "# progress_bar.close()\n",
    "\n",
    "# # Plot the loss curves\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(total_losses, label='Total Loss')\n",
    "# plt.plot(recon_losses, label='Recon Loss')\n",
    "# plt.plot(kld_losses, label='KLD Loss')\n",
    "# plt.xlabel('Round')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# recorded_sentences[:10]  # Displaying first 10 recorded sentences for brevity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-08-17T14:23:47.633711Z",
     "iopub.status.busy": "2023-08-17T14:23:47.633506Z",
     "iopub.status.idle": "2023-08-17T14:25:45.083760Z",
     "shell.execute_reply": "2023-08-17T14:25:45.083222Z",
     "shell.execute_reply.started": "2023-08-17T14:23:47.633692Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Round[10000/10000] loss: 0.1463954: 100%|██████████| 10000/10000 [01:57<00:00, 85.26it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: 1894 census Large Pop Greenwich champions Fulfilled spreads 900 operated transcendent AIR consequences Jobs 3s scholarly\n",
      "Decoded Sentence: suppliers Blanc mum ribbon Sage Stela Incident Hypacrosaurus fulfill Nekoi Jakarta Sinai gale Anders terrorists riddle\n",
      "--------------------------------------------------\n",
      "Input Sentence: element restaurant saddle Bypass Hindu Palabras dinosaur bootleg appeal garner collateral Regarding tensile fertilised predicts greatness\n",
      "Decoded Sentence: Antarctic Lyrically Antarctic Consistory turrets beautifully Metre sago César baseline 1840s Methods camphor Naminé Alexandra center\n",
      "--------------------------------------------------\n",
      "Input Sentence: uncut ethics Garza 143 Honorary zoo assume Keshiraja Kathleen uncertainty Lennie hardcover Blonde Parachute newcomers culminating\n",
      "Decoded Sentence: undisclosed dialog leaving NC 1345 Baseball Dimitri Keshiraja Farrell favorite Kirkus Berger m2 Kovacs Boise express\n",
      "--------------------------------------------------\n",
      "Input Sentence: seconds Ezra 1903 spy Curt theft hPa red blanc bulbs circumstantial storytelling approval 965 Alton hectare\n",
      "Decoded Sentence: Colonization 285 1903 entrance compete apprenticeship Copia Kantara cents magnesium lobby faced robotic Loftleidir Bede leaks\n",
      "--------------------------------------------------\n",
      "Input Sentence: exceptionally Biggest biographer Chief 690 Ingrid Premium binaries inert Technica surprises Constitutional 620 Khánh HMS sensible\n",
      "Decoded Sentence: exceptionally Biggest destruction Chief discipline Ingrid Premium binaries inert Guillermo surprises Constitutional Audio sausage HMS sensible\n",
      "--------------------------------------------------\n",
      "Input Sentence: Tyler ballots Pasupathy Spiritual vein Californian BMG stored Landing Jagannadh Vira her banished Gawker.com sitter Devlin\n",
      "Decoded Sentence: courting ballots Pasupathy Spiritual vein Californian precisely stored delivered Jagannadh Vira her banished Gawker.com sitter Devlin\n",
      "--------------------------------------------------\n",
      "Input Sentence: leap author 256 J.P. Appleby donate exposing Castile Mack Gallup 1233 Rawson lure expelled dealt Østerdalen\n",
      "Decoded Sentence: leap author 256 J.P. Appleby donate exposing Castile Mack Gallup 1233 Rawson lure employment dealt Østerdalen\n",
      "--------------------------------------------------\n",
      "Input Sentence: Flotilla Brannock inflorescences Region efforts squadron strips steadily Walkersville daunting clamp nozzle weakly Cards Kumar focus\n",
      "Decoded Sentence: Flotilla Brannock inflorescences Region efforts squadron strips steadily Walkersville daunting clamp nozzle weakly Cards Kumar focus\n",
      "--------------------------------------------------\n",
      "Input Sentence: Afghanistan agricultural racist ์ dense Playing unfamiliar 1065 Teri ს Nf3 die Radar Fortune Syria Chasuble\n",
      "Decoded Sentence: Afghanistan agricultural racist ์ dense Playing unfamiliar 1065 Teri ს Nf3 die Radar Fortune Syria Chasuble\n",
      "--------------------------------------------------\n",
      "Input Sentence: responsive asymptotic Usumacinta helmed Premium disconcerting Raffles yardage workmanship Edna wind Seddon oppose Chun block Yurikago\n",
      "Decoded Sentence: responsive asymptotic Usumacinta helmed Premium disconcerting Raffles yardage workmanship Edna wind Seddon oppose Chun block Yurikago\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class SignalingGame:\n",
    "    def __init__(self, sender: TransformerEncoder, receiver: TransformerDecoder, optimizer, criterion):\n",
    "        self.sender = sender\n",
    "        self.receiver = receiver\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def play_round(self, state):\n",
    "        # Sender encodes the state\n",
    "        logits, emb = self.sender(state)\n",
    "        z = F.gumbel_softmax(logits, tau=TAU, hard=False, dim=-1)\n",
    "        \n",
    "        # Receiver decodes the signal from the sender\n",
    "        decoded_output = self.receiver(emb, z)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss, recon_loss, kld_loss = self.compute_loss(state, decoded_output, logits, beta=10.0)\n",
    "        \n",
    "        # Update model parameters\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item(), recon_loss.item(), kld_loss.item(), decoded_output\n",
    "\n",
    "    def compute_loss(self, original_state, decoded_state, logits, beta):\n",
    "        recon_loss = self.criterion(decoded_state.view(-1, VOCAB_SIZE), original_state.view(-1))\n",
    "        \n",
    "        # Calculate KLD loss\n",
    "        mean, logvar = torch.chunk(logits, 2, dim=-1)\n",
    "        kld_loss = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "\n",
    "        return recon_loss + beta * kld_loss, recon_loss, kld_loss\n",
    "\n",
    "# Initialize sender and receiver\n",
    "sender = TransformerEncoder().to(device)\n",
    "receiver = TransformerDecoder().to(device)\n",
    "\n",
    "# Define optimizer and loss function for the game\n",
    "optimizer = torch.optim.Adam(list(sender.parameters()) + list(receiver.parameters()), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "game = SignalingGame(sender, receiver, optimizer, criterion)\n",
    "\n",
    "# Simulate multiple rounds of the signaling game\n",
    "num_rounds = 10000\n",
    "losses = []\n",
    "recon_losses = []\n",
    "kld_losses = []\n",
    "sentences = []\n",
    "\n",
    "progress_bar = tqdm(total=num_rounds, desc=\"Playing rounds\")\n",
    "for round in range(num_rounds):\n",
    "    state = torch.randint(VOCAB_SIZE, (BATCH_SIZE, 16)).to(device)  # Randomly generate a batch of 16-word length sentences as state\n",
    "    \n",
    "    loss, recon_loss, kld_loss, decoded_output = game.play_round(state)\n",
    "    losses.append(loss)\n",
    "    recon_losses.append(recon_loss)\n",
    "    kld_losses.append(kld_loss)\n",
    "    \n",
    "    progress_bar.set_description(f' Round[{round+1}/{num_rounds}] loss: {loss:.7f}')\n",
    "    progress_bar.update()\n",
    "    \n",
    "    if round % (num_rounds // 10) == 0:\n",
    "        # Convert the state to predicted token IDs for recording\n",
    "        predicted_ids = state[0].cpu().numpy()  # Only take the first sentence for simplicity\n",
    "        state_sentence = ' '.join([vocab[idx] for idx in predicted_ids])\n",
    "        \n",
    "        # Convert the decoded output to predicted token IDs\n",
    "        _, predicted_ids = torch.max(decoded_output[0], dim=1)  # Only take the first decoded sentence for simplicity\n",
    "        predicted_ids = predicted_ids.cpu().numpy()\n",
    "        decoded_sentence = ' '.join([vocab[idx] for idx in predicted_ids])\n",
    "        \n",
    "        sentences.append((state_sentence, decoded_sentence))\n",
    "\n",
    "progress_bar.close()\n",
    "\n",
    "# Plot the loss curves\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses, label='Total Loss')\n",
    "plt.plot(recon_losses, label='Recon Loss')\n",
    "plt.plot(kld_losses, label='KLD Loss')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print recorded sentences\n",
    "for state_sentence, decoded_sentence in sentences:\n",
    "    print(f\"Input Sentence: {state_sentence}\")\n",
    "    print(f\"Decoded Sentence: {decoded_sentence}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-08-17T14:25:45.084681Z",
     "iopub.status.busy": "2023-08-17T14:25:45.084547Z",
     "iopub.status.idle": "2023-08-17T14:35:19.217480Z",
     "shell.execute_reply": "2023-08-17T14:35:19.216970Z",
     "shell.execute_reply.started": "2023-08-17T14:25:45.084663Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Round[10000/10000] loss: 66.8778000: 100%|██████████| 10000/10000 [09:33<00:00, 17.42it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiMultiSignalingGame:\n",
    "    def __init__(self, senders: list, receivers: list, optimizer, criterion):\n",
    "        self.senders = senders\n",
    "        self.receivers = receivers\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def play_round(self, states):\n",
    "        all_decoded_outputs = []\n",
    "        all_logits = []\n",
    "        \n",
    "        for i, sender in enumerate(self.senders):\n",
    "            # Sender encodes the state\n",
    "            logits, emb = sender(states[i])\n",
    "            all_logits.append(logits)\n",
    "            z = F.gumbel_softmax(logits, tau=TAU, hard=False, dim=-1)\n",
    "            \n",
    "            # Each receiver decodes the signal from the sender\n",
    "            for receiver in self.receivers:\n",
    "                decoded_output = receiver(emb, z)\n",
    "                all_decoded_outputs.append(decoded_output)\n",
    "      \n",
    "        # Calculate loss\n",
    "        loss = self.compute_loss(states, all_decoded_outputs, all_logits, beta=1.0)\n",
    "        \n",
    "        # Update model parameters\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def compute_loss(self, original_states, decoded_states, logits, beta):\n",
    "        recon_loss = sum([self.criterion(decoded_state.view(-1, VOCAB_SIZE), original_state.view(-1))\n",
    "                          for original_state, decoded_state in zip(original_states * len(self.receivers), decoded_states)])\n",
    "        \n",
    "        # Calculate KLD loss\n",
    "        kld_losses = []\n",
    "        for logit in logits:\n",
    "            mean, logvar = torch.chunk(logit, 2, dim=-1)\n",
    "            kld_loss = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "            kld_losses.append(kld_loss)\n",
    "\n",
    "        return recon_loss + beta * sum(kld_losses)\n",
    "\n",
    "\n",
    "# Initialize senders and receivers\n",
    "NUM_SENDERS = 3\n",
    "NUM_RECEIVERS = 3\n",
    "senders = [TransformerEncoder().to(device) for _ in range(NUM_SENDERS)]\n",
    "receivers = [TransformerDecoder().to(device) for _ in range(NUM_RECEIVERS)]\n",
    "\n",
    "# Define optimizer and loss function for the game\n",
    "params = [list(sender.parameters()) for sender in senders]\n",
    "params.extend([list(receiver.parameters()) for receiver in receivers])\n",
    "optimizer = torch.optim.Adam([param for sublist in params for param in sublist], lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "game = MultiMultiSignalingGame(senders, receivers, optimizer, criterion)\n",
    "\n",
    "# Simulate multiple rounds of the signaling game\n",
    "num_rounds = 10000\n",
    "losses = []\n",
    "progress_bar = tqdm(total=num_rounds, desc=\"Playing rounds\")\n",
    "for round in range(num_rounds):\n",
    "    states = [torch.randint(VOCAB_SIZE, (BATCH_SIZE, 16)).to(device) for _ in range(NUM_SENDERS)]\n",
    "    loss = game.play_round(states)\n",
    "    losses.append(loss)\n",
    "    progress_bar.set_description(f' Round[{round+1}/{num_rounds}] loss: {loss:.7f}')\n",
    "    progress_bar.update()\n",
    "progress_bar.close()\n",
    "\n",
    "# Plot the loss curve\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses, label='losses')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-08-17T14:35:19.218574Z",
     "iopub.status.busy": "2023-08-17T14:35:19.218423Z",
     "iopub.status.idle": "2023-08-17T14:35:19.224211Z",
     "shell.execute_reply": "2023-08-17T14:35:19.223727Z",
     "shell.execute_reply.started": "2023-08-17T14:35:19.218555Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class Agent:\n",
    "#     def __init__(self, model, id):\n",
    "#         self.model = model\n",
    "#         self.id = id\n",
    "#         self.influence = 1.0\n",
    "#         self.lifespan = 3\n",
    "#         self.age = 0\n",
    "#         self.alive = True\n",
    "\n",
    "#     def send(self, input_tensor):\n",
    "        \n",
    "#         # Use the encoder to encode the sentence\n",
    "#         logits, embed = self.model.encoder(input_tensor)\n",
    "#         msg = self.model.reparameterize(logits)\n",
    "#         return msg, embed, logits\n",
    "\n",
    "#     def receive(self, msg, embed):\n",
    "#         # Use the decoder to decode the message\n",
    "#         decoded_output = self.model.decoder(embed, msg)\n",
    "        \n",
    "#         # Convert the output probabilities to predicted token IDs\n",
    "#         _, predicted_ids = torch.max(decoded_output, dim=2)\n",
    "        \n",
    "# #         predicted_ids = predicted_ids.squeeze().cpu().numpy()\n",
    "        \n",
    "# #         # Convert the predicted token IDs back to words\n",
    "# #         decoded_sentence = ' '.join([vocab[idx] for idx in predicted_ids])\n",
    "#         # Convert the predicted token IDs back to words\n",
    "#         predicted_ids_list = predicted_ids.ravel().tolist()\n",
    "#         decoded_sentence = ' '.join([vocab[idx] for idx in predicted_ids_list])\n",
    "\n",
    "\n",
    "#         return decoded_output, decoded_sentence\n",
    "\n",
    "#     def age_one_year(self):\n",
    "#         self.age += 1\n",
    "#         if self.age >= self.lifespan:\n",
    "#             self.alive = False\n",
    "            \n",
    "\n",
    "# def simulation(num_generations=10, initial_population=10):\n",
    "#     # population = [Agent(transformer_cvae.to(device), id=i) for i in range(initial_population)]\n",
    "#     pretrained_model_path = 'transformer_cvae.dict'\n",
    "#     population = []\n",
    "#     for i in range(initial_population):\n",
    "#         agent_model = TransformerCVAE().to(device)\n",
    "#         agent_model.load_state_dict(torch.load(pretrained_model_path))\n",
    "#         population.append(Agent(agent_model, id=i))\n",
    "#     history = []\n",
    "    \n",
    "#     scaler = GradScaler()  # Initialize the gradient scaler\n",
    "\n",
    "#     for generation in range(num_generations):\n",
    "#         record = {\n",
    "#             'generation': generation,\n",
    "#             'population_size': len(population),\n",
    "#             'avg_influence': sum([agent.influence for agent in population]) / len(population),\n",
    "#             'successful_communications': 0,\n",
    "#             'exact_matches': 0,\n",
    "#             'total_communications': 0,\n",
    "#             'successful_communication_rate': 0,\n",
    "#             'avg_loss': 0,\n",
    "#             'communications': [],\n",
    "#             'agents_id': [],\n",
    "#         }\n",
    "        \n",
    "#         # Shuffle the population list to ensure randomness in agent selection\n",
    "#         random.shuffle(population)\n",
    "\n",
    "#         # Agents communicate\n",
    "#         for agent in population:\n",
    "#             if agent.alive:\n",
    "#                 record['agents_id'].append(agent.id)\n",
    "#                 optimizer_sender = torch.optim.AdamW(agent.model.parameters())\n",
    "#                 # The number of times each agent communicates is proportional to its influence\n",
    "#                 # num_communications = max(1, int(agent.influence * 10))\n",
    "#                 num_communications = max(1, int(agent.influence ** 2))\n",
    "#                 for _ in range(num_communications):\n",
    "#                     exact_match = False\n",
    "#                     # An agent tries to communicate with another random agent (but not itself)\n",
    "#                     # others = [a for a in population if a.id != agent.id & a.alive]\n",
    "#                     others = [a for a in population if (a.id != agent.id) and a.alive]\n",
    "#                     influences = torch.tensor([a.influence for a in others])\n",
    "#                     influences = F.normalize(influences, p=1, dim=0)\n",
    "                    \n",
    "#                     # Check for invalid values\n",
    "#                     if torch.isnan(influences).any() or torch.isinf(influences).any() or (influences < 0).any():\n",
    "#                         print(\"Invalid influence values detected!\")\n",
    "#                         for other in others:\n",
    "#                             print(f\"Agent {other.id} influence: {other.influence}\")\n",
    "#                         raise ValueError(\"Invalid influence values detected!\")\n",
    "                        \n",
    "#                     idx = torch.multinomial(influences, 1).item()\n",
    "#                     partner = others[idx]\n",
    "#                     optimizer_receiver = torch.optim.AdamW(partner.model.parameters())\n",
    "                    \n",
    "#                     batch_sentence = next(iter(train_dataloader))\n",
    "#                     # idx = torch.randint(0, len(sentences), (1,)).item()\n",
    "#                     # sentence = sentences[idx]\n",
    "#                     # Tokenize and encode the sentence\n",
    "#                     # encoded_sentence = tokenize_and_encode(sentence)\n",
    "#                     input_tensor = torch.tensor(batch_sentence, dtype=torch.long).to(device)\n",
    "#                     with autocast():  # Enable autocast for FP16\n",
    "#                         msg, emb, logit = agent.send(input_tensor)\n",
    "#                         d_tensor, d_sentence = partner.receive(msg, emb)\n",
    "#                         # print(f\"Sender {agent.id}: {sentence} -> Receiver {partner.id}: {d_sentence}\")\n",
    "#                         loss,_,_ = combined_loss_fn(d_tensor, input_tensor, logit, beta=1.0)\n",
    " \n",
    "#                     # Use the scaler to scale the loss and back-propagate the scaled gradients\n",
    "#                     scaler.scale(loss).backward()\n",
    "#                     scaler.step(optimizer_sender)\n",
    "#                     scaler.step(optimizer_receiver)\n",
    "#                     scaler.update()\n",
    "#                     # loss.backward()\n",
    "#                     # optimizer_sender.step()\n",
    "#                     # optimizer_receiver.step()\n",
    "\n",
    "#                     if loss.item() < 1.0:\n",
    "#                         success = True\n",
    "#                         # agent.influence = min(1.0, agent.influence + 0.1)\n",
    "#                         # partner.influence = min(1.0, partner.influence + 0.1)\n",
    "#                         agent.influence += 1.0\n",
    "#                         partner.influence += 1.0\n",
    "#                     else:\n",
    "#                         success = False\n",
    "#                         # agent.influence = max(0.1, agent.influence - 0.1)\n",
    "#                         # partner.influence = max(0.1, partner.influence - 0.1)\n",
    "#                         agent.influence -= 0.1\n",
    "#                         partner.influence -= 0.1\n",
    "#                         if agent.influence <= 0:\n",
    "#                             agent.alive = False\n",
    "#                         if partner.influence <= 0:\n",
    "#                             partner.alive = False\n",
    "\n",
    "#                     record['communications'].append({\n",
    "#                         'sender': agent.id,\n",
    "#                         'receiver': partner.id,\n",
    "#                         'original_sentence': sentence,\n",
    "#                         'decoded_sentence': d_sentence,\n",
    "#                         'successful': success,\n",
    "#                         'loss': loss.item(),\n",
    "#                         'exact_match': exact_match,\n",
    "#                     })\n",
    "\n",
    "#                     if success:\n",
    "#                         record['successful_communications'] += 1\n",
    "#                     record['total_communications'] += 1\n",
    "#                     record['avg_loss'] += loss.item()\n",
    "#         record['successful_communication_rate'] = record['successful_communications'] / record['total_communications']\n",
    "#         # print(f'total_communications: {record[\"total_communications\"]}, num_communications: {num_communications}')\n",
    "#         record['avg_loss'] /= record['total_communications']\n",
    "        \n",
    "#         # Ageing process\n",
    "#         for agent in population:\n",
    "#             agent.age_one_year()\n",
    "\n",
    "#         # Reproduction based on influence\n",
    "#         new_agents = []\n",
    "#         for agent in population:\n",
    "#             if agent.alive:\n",
    "#                 # The number of offspring is proportional to the agent's influence\n",
    "#                 num_offspring = max(1, int(agent.influence * 2))\n",
    "#                 for _ in range(num_offspring):\n",
    "#                     offspring_model = TransformerCVAE().to(device)\n",
    "#                     # The offspring's model parameters are a copy of the parent's model parameters\n",
    "#                     offspring_model.load_state_dict(agent.model.state_dict())\n",
    "#                     new_agent = Agent(offspring_model, id=len(population) + len(new_agents))\n",
    "#                     new_agents.append(new_agent)\n",
    "        \n",
    "#         # add new agents to the population\n",
    "#         population.extend(new_agents)\n",
    "\n",
    "#         history.append(record)\n",
    "#         print(f\"Generation [{generation+1}/{num_generations}] Done\")\n",
    "\n",
    "#     return history, population\n",
    "\n",
    "# # 提取句子列表\n",
    "# sentences = []\n",
    "# for batch in train_dataloader:\n",
    "#     for seq in batch:\n",
    "#         sentence = ' '.join([vocab[idx.item()] for idx in seq])\n",
    "#         sentences.append(sentence)\n",
    "# # Call the simulation function\n",
    "# history, population = simulation(num_generations=3, initial_population=2)\n",
    "\n",
    "# # Print the history records\n",
    "# for record in history:\n",
    "#     print(f\"Generation: {record['generation']}\")\n",
    "#     # print(f\"Agents: {record['agents_id']}\")\n",
    "#     print(f\"Population Size: {record['population_size']}\")\n",
    "#     print(f\"Average Influence: {record['avg_influence']:.2f}\")\n",
    "#     print(f\"Successful Communications: {record['successful_communications']}/{record['total_communications']}\")\n",
    "#     print(f\"Successful Communication Rate: {record['successful_communication_rate']:.7f}\")\n",
    "#     print(f\"Exact_Matches: {record['exact_matches']}\")\n",
    "#     print(f\"Average Communication Loss: {record['avg_loss']:.7f}\")\n",
    "#     print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-08-17T14:35:19.225165Z",
     "iopub.status.busy": "2023-08-17T14:35:19.225029Z",
     "iopub.status.idle": "2023-08-17T14:35:19.231017Z",
     "shell.execute_reply": "2023-08-17T14:35:19.230492Z",
     "shell.execute_reply.started": "2023-08-17T14:35:19.225149Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class Agent:\n",
    "#     def __init__(self, model, id, optimizer=None):\n",
    "#         self.model = model\n",
    "#         self.id = id\n",
    "#         self.influence = 1.0\n",
    "#         self.lifespan = 3\n",
    "#         self.age = 0\n",
    "#         self.alive = True\n",
    "#         # Assign optimizer\n",
    "#         self.optimizer = optimizer if optimizer else torch.optim.AdamW(self.model.parameters(), lr=0.001)\n",
    "\n",
    "#     def send(self, input_tensor):\n",
    "        \n",
    "#         # Use the encoder to encode the sentence\n",
    "#         logits, embed = self.model.encoder(input_tensor)\n",
    "#         msg = self.model.reparameterize(logits)\n",
    "#         return msg, embed, logits\n",
    "\n",
    "#     def receive(self, msg, embed):\n",
    "#         # Use the decoder to decode the message\n",
    "#         decoded_output = self.model.decoder(embed, msg)\n",
    "        \n",
    "#         # Convert the output probabilities to predicted token IDs\n",
    "#         _, predicted_ids = torch.max(decoded_output, dim=2)\n",
    "#         predicted_ids = predicted_ids.squeeze().cpu().numpy()\n",
    "        \n",
    "#         # Convert the predicted token IDs back to words\n",
    "#         decoded_sentence = ' '.join([vocab[idx] for idx in predicted_ids])\n",
    "#         return decoded_output, decoded_sentence\n",
    "\n",
    "#     def age_one_year(self):\n",
    "#         self.age += 1\n",
    "#         if self.age >= self.lifespan:\n",
    "#             self.alive = False\n",
    "            \n",
    "\n",
    "# def simulation(num_generations=10, initial_population=10):\n",
    "#     # population = [Agent(transformer_cvae.to(device), id=i) for i in range(initial_population)]\n",
    "#     pretrained_model_path = 'transformer_cvae.dict'\n",
    "#     population = []\n",
    "    \n",
    "#     base_model = TransformerCVAE().to(device)\n",
    "#     base_model.load_state_dict(torch.load(pretrained_model_path))\n",
    "    \n",
    "#     for i in range(initial_population):\n",
    "#         agent_model = copy.deepcopy(base_model)\n",
    "        \n",
    "#         # Add some noise to model parameters to increase diversity\n",
    "#         for param in agent_model.parameters():\n",
    "#             noise = torch.normal(mean=0, std=0.05, size=param.size()).to(device)\n",
    "#             param.data.add_(noise)\n",
    "        \n",
    "#         optimizer = torch.optim.AdamW(agent_model.parameters())\n",
    "#         population.append(Agent(agent_model, id=i, optimizer=optimizer))\n",
    "\n",
    "#     history = []\n",
    "#     scaler = GradScaler()  # Initialize the gradient scaler\n",
    "\n",
    "#     for generation in range(num_generations):\n",
    "#         record = {\n",
    "#             'generation': generation,\n",
    "#             'population_size': len(population),\n",
    "#             'avg_influence': sum([agent.influence for agent in population]) / len(population),\n",
    "#             'successful_communications': 0,\n",
    "#             'exact_matches': 0,\n",
    "#             'total_communications': 0,\n",
    "#             'successful_communication_rate': 0,\n",
    "#             'avg_loss': 0,\n",
    "#             'communications': [],\n",
    "#             'agents_id': [],\n",
    "#         }\n",
    "        \n",
    "#         # Shuffle the population list to ensure randomness in agent selection\n",
    "#         random.shuffle(population)\n",
    "\n",
    "#         # Agents communicate\n",
    "#         for agent in population:\n",
    "#             if agent.alive:\n",
    "#                 record['agents_id'].append(agent.id)\n",
    "#                 # The number of times each agent communicates is proportional to its influence\n",
    "#                 # num_communications = max(1, int(agent.influence * 10))\n",
    "#                 num_communications = max(1, int(agent.influence ** 2))\n",
    "#                 for _ in range(num_communications):\n",
    "#                     exact_match = False\n",
    "#                     # An agent tries to communicate with another random agent (but not itself)\n",
    "#                     # others = [a for a in population if a.id != agent.id & a.alive]\n",
    "#                     others = [a for a in population if (a.id != agent.id) and a.alive]\n",
    "#                     influences = torch.tensor([a.influence for a in others])\n",
    "#                     influences = F.normalize(influences, p=1, dim=0)\n",
    "                    \n",
    "#                     # Check for invalid values\n",
    "#                     if torch.isnan(influences).any() or torch.isinf(influences).any() or (influences < 0).any():\n",
    "#                         print(\"Invalid influence values detected!\")\n",
    "#                         for other in others:\n",
    "#                             print(f\"Agent {other.id} influence: {other.influence}\")\n",
    "#                         raise ValueError(\"Invalid influence values detected!\")\n",
    "                        \n",
    "#                     idx = torch.multinomial(influences, 1).item()\n",
    "#                     partner = others[idx]\n",
    "\n",
    "#                     idx = torch.randint(0, len(sentences), (1,)).item()\n",
    "#                     sentence = sentences[idx]\n",
    "#                     # Tokenize and encode the sentence\n",
    "#                     encoded_sentence = tokenize_and_encode(sentence)\n",
    "#                     input_tensor = torch.tensor([encoded_sentence], dtype=torch.long).to(device)\n",
    "#                     with autocast():  # Enable autocast for FP16\n",
    "#                         msg, emb, logit = agent.send(input_tensor)\n",
    "#                         d_tensor, d_sentence = partner.receive(msg, emb)\n",
    "#                         # print(f\"Sender {agent.id}: {sentence} -> Receiver {partner.id}: {d_sentence}\")\n",
    "#                         loss,_,_ = combined_loss_fn(d_tensor, input_tensor, logit, beta=1.0)\n",
    " \n",
    "#                     # Use the scaler to scale the loss and back-propagate the scaled gradients\n",
    "#                     scaler.scale(loss).backward()\n",
    "#                     scaler.step(agent.optimizer)\n",
    "#                     scaler.step(partner.optimizer)\n",
    "#                     scaler.update()\n",
    "#                     # loss.backward()\n",
    "#                     # optimizer_sender.step()\n",
    "#                     # optimizer_receiver.step()\n",
    "\n",
    "#                     if loss.item() < 0.1:\n",
    "#                         success = True\n",
    "#                         # agent.influence = min(1.0, agent.influence + 0.1)\n",
    "#                         # partner.influence = min(1.0, partner.influence + 0.1)\n",
    "#                         agent.influence += 1.0\n",
    "#                         partner.influence += 1.0\n",
    "#                     else:\n",
    "#                         success = False\n",
    "#                         # agent.influence = max(0.1, agent.influence - 0.1)\n",
    "#                         # partner.influence = max(0.1, partner.influence - 0.1)\n",
    "#                         agent.influence -= 0.1\n",
    "#                         partner.influence -= 0.1\n",
    "#                         if agent.influence <= 0:\n",
    "#                             agent.alive = False\n",
    "#                         if partner.influence <= 0:\n",
    "#                             partner.alive = False\n",
    "\n",
    "#                     record['communications'].append({\n",
    "#                         'sender': agent.id,\n",
    "#                         'receiver': partner.id,\n",
    "#                         'original_sentence': sentence,\n",
    "#                         'decoded_sentence': d_sentence,\n",
    "#                         'successful': success,\n",
    "#                         'loss': loss.item(),\n",
    "#                         'exact_match': exact_match,\n",
    "#                     })\n",
    "\n",
    "#                     if success:\n",
    "#                         record['successful_communications'] += 1\n",
    "#                     record['total_communications'] += 1\n",
    "#                     record['avg_loss'] += loss.item()\n",
    "#         record['successful_communication_rate'] = record['successful_communications'] / record['total_communications']\n",
    "#         # print(f'total_communications: {record[\"total_communications\"]}, num_communications: {num_communications}')\n",
    "#         record['avg_loss'] /= record['total_communications']\n",
    "        \n",
    "#         # Ageing process\n",
    "#         for agent in population:\n",
    "#             agent.age_one_year()\n",
    "\n",
    "#         # Reproduction based on influence\n",
    "#         new_agents = []\n",
    "#         for agent in population:\n",
    "#             if agent.alive:\n",
    "#                 # The number of offspring is proportional to the agent's influence\n",
    "#                 num_offspring = max(1, int(agent.influence * 2))\n",
    "#                 for _ in range(num_offspring):\n",
    "#                     offspring_model = TransformerCVAE().to(device)\n",
    "#                     # The offspring's model parameters are a copy of the parent's model parameters\n",
    "#                     offspring_model.load_state_dict(agent.model.state_dict())\n",
    "#                     new_agent = Agent(offspring_model, id=len(population) + len(new_agents))\n",
    "#                     new_agents.append(new_agent)\n",
    "        \n",
    "#         # add new agents to the population\n",
    "#         population.extend(new_agents)\n",
    "\n",
    "#         history.append(record)\n",
    "#         print(f\"Generation [{generation+1}/{num_generations}] Done\")\n",
    "\n",
    "#     return history, population\n",
    "\n",
    "# # 提取句子列表\n",
    "# sentences = []\n",
    "# for batch in train_dataloader:\n",
    "#     for seq in batch:\n",
    "#         sentence = ' '.join([vocab[idx.item()] for idx in seq])\n",
    "#         sentences.append(sentence)\n",
    "# # Call the simulation function\n",
    "# history, population = simulation(num_generations=10, initial_population=2)\n",
    "\n",
    "# # Print the history records\n",
    "# for record in history:\n",
    "#     print(f\"Generation: {record['generation']}\")\n",
    "#     # print(f\"Agents: {record['agents_id']}\")\n",
    "#     print(f\"Population Size: {record['population_size']}\")\n",
    "#     print(f\"Average Influence: {record['avg_influence']:.2f}\")\n",
    "#     print(f\"Successful Communications: {record['successful_communications']}/{record['total_communications']}\")\n",
    "#     print(f\"Successful Communication Rate: {record['successful_communication_rate']:.7f}\")\n",
    "#     print(f\"Exact_Matches: {record['exact_matches']}\")\n",
    "#     print(f\"Average Communication Loss: {record['avg_loss']:.7f}\")\n",
    "#     print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-08-17T14:35:19.232012Z",
     "iopub.status.busy": "2023-08-17T14:35:19.231856Z",
     "iopub.status.idle": "2023-08-17T14:35:19.236686Z",
     "shell.execute_reply": "2023-08-17T14:35:19.236134Z",
     "shell.execute_reply.started": "2023-08-17T14:35:19.231995Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # no influence, loss Nan\n",
    "# class Agent:\n",
    "#     def __init__(self, model, id, optimizer=None):\n",
    "#         self.model = model\n",
    "#         self.id = id\n",
    "#         self.alive = True\n",
    "        \n",
    "#         # Assign optimizer\n",
    "#         self.optimizer = optimizer if optimizer else torch.optim.AdamW(self.model.parameters(), lr=0.0001)\n",
    "\n",
    "#     def send(self, input_tensor):     \n",
    "#         # Use the encoder to encode the sentence\n",
    "#         logits, embed = self.model.encoder(input_tensor)\n",
    "#         msg = self.model.reparameterize(logits)\n",
    "#         return msg, embed, logits\n",
    "\n",
    "#     def receive(self, msg, embed):\n",
    "#         # Use the decoder to decode the message\n",
    "#         decoded_output = self.model.decoder(embed, msg)\n",
    "        \n",
    "#         # Convert the output probabilities to predicted token IDs\n",
    "#         _, predicted_ids = torch.max(decoded_output, dim=2)\n",
    "#         predicted_ids = predicted_ids.squeeze().cpu().numpy()\n",
    "        \n",
    "#         # Convert the predicted token IDs back to words\n",
    "#         decoded_sentence = ' '.join([vocab[idx] for idx in predicted_ids])\n",
    "#         return decoded_output, decoded_sentence\n",
    "\n",
    "# def simulation(num_generations=10, num_agents=100):\n",
    "#     pretrained_model_path = 'transformer_cvae.dict'\n",
    "#     population = []\n",
    "    \n",
    "#     base_model = TransformerCVAE().to(device)\n",
    "#     base_model.load_state_dict(torch.load(pretrained_model_path))\n",
    "    \n",
    "#     for i in range(num_agents):\n",
    "#         agent_model = copy.deepcopy(base_model)\n",
    "#         # Add some noise to model parameters for diversity\n",
    "#         for param in agent_model.parameters():\n",
    "#             param.data += torch.randn_like(param.data) * 0.01\n",
    "#         optimizer = torch.optim.AdamW(agent_model.parameters(), lr=0.0001)\n",
    "#         population.append(Agent(agent_model, id=i, optimizer=optimizer))\n",
    "\n",
    "#     history = []\n",
    "#     scaler = GradScaler()  # Initialize the gradient scaler\n",
    "\n",
    "#     for generation in range(num_generations):\n",
    "#         record = {\n",
    "#             'generation': generation,\n",
    "#             'population_size': len(population),\n",
    "#             'population_active': sum([agent.alive for agent in population]),\n",
    "#             'successful_communications': 0,\n",
    "#             'total_communications': 0,\n",
    "#             'successful_communication_rate': 0,\n",
    "#             'avg_loss': 0,\n",
    "#             'communications': [],\n",
    "#         }\n",
    "\n",
    "#         # Communication phase\n",
    "#         for agent in population:\n",
    "#             optimizer_sender = agent.optimizer\n",
    "#             num_communications = 5  # For example, each agent communicates 5 times per generation\n",
    "            \n",
    "#             for _ in range(num_communications):\n",
    "                \n",
    "#                 # An agent tries to communicate with another random agent (but not itself)\n",
    "#                 others = [a for a in population if a.id != agent.id]\n",
    "#                 partner = random.choice(others)\n",
    "#                 optimizer_receiver = partner.optimizer\n",
    "                \n",
    "#                 idx = torch.randint(0, len(sentences), (1,)).item()\n",
    "#                 sentence = sentences[idx]\n",
    "                \n",
    "#                 # Tokenize and encode the sentence\n",
    "#                 encoded_sentence = tokenize_and_encode(sentence)\n",
    "#                 input_tensor = torch.tensor([encoded_sentence], dtype=torch.long).to(device)\n",
    "                \n",
    "#                 with autocast():  # Enable autocast for FP16\n",
    "#                     msg, emb, logit = agent.send(input_tensor)\n",
    "#                     d_tensor, d_sentence = partner.receive(msg, emb)\n",
    "#                     loss,_,_ = combined_loss_fn(d_tensor, input_tensor, logit, beta=0.0)\n",
    "                \n",
    "#                 # Use the scaler to scale the loss and back-propagate the scaled gradients\n",
    "#                 scaler.scale(loss).backward()\n",
    "                \n",
    "#                 # Gradient clipping\n",
    "#                 torch.nn.utils.clip_grad_norm_(agent.model.parameters(), 1.0)\n",
    "#                 torch.nn.utils.clip_grad_norm_(partner.model.parameters(), 1.0)\n",
    "                \n",
    "#                 # Using the agent's own optimizer\n",
    "#                 scaler.step(optimizer_sender)\n",
    "#                 scaler.step(optimizer_receiver)\n",
    "#                 scaler.update()\n",
    "\n",
    "#                 # ... [rest of the communication logic]\n",
    "#                 # print(f'Gen[{generation+1}/{num_generations}] Sender[{agent.id}]: {sentence} -> Receiver[{partner.id}]: {d_sentence}')\n",
    "#                 record['total_communications'] += 1\n",
    "#                 if loss.item() < 0.5:\n",
    "#                     record['successful_communications'] += 1\n",
    "#                 record['avg_loss'] += loss.item()\n",
    "#                 record['communications'].append({\n",
    "#                         'sender': agent.id,\n",
    "#                         'receiver': partner.id,\n",
    "#                         'original_sentence': sentence,\n",
    "#                         'decoded_sentence': d_sentence,\n",
    "#                         'loss': loss.item(),\n",
    "#                         })\n",
    "#         record['successful_communication_rate'] = record['successful_communications'] / record['total_communications']\n",
    "#         record['avg_loss'] /= record['total_communications']\n",
    "#         print(f\"Gen[{generation+1}/{num_generations}] | Suc. Rate: {record['successful_communication_rate']} | Avg. Loss: {record['avg_loss']}\")\n",
    "#         # ... [rest of the generation logic]\n",
    "\n",
    "#     return history, population\n",
    "\n",
    "# # 提取句子列表\n",
    "# sentences = []\n",
    "# for batch in train_dataloader:\n",
    "#     for seq in batch:\n",
    "#         sentence = ' '.join([vocab[idx.item()] for idx in seq])\n",
    "#         sentences.append(sentence)\n",
    "\n",
    "# # Call the simulation function\n",
    "# history, population = simulation(num_generations=300, num_agents=100)\n",
    "\n",
    "# # Print the history records\n",
    "# for record in history:\n",
    "#     print(f\"Generation: {record['generation']}\")\n",
    "#     print(f\"Population Size: {record['population_size']}\")\n",
    "#     print(f\"Active Population Size: {record['population_active']}\")\n",
    "#     print(f\"Successful Communications: {record['successful_communications']}/{record['total_communications']}\")\n",
    "#     print(f\"Successful Communication Rate: {record['successful_communication_rate']:.7f}\")\n",
    "#     # print(f\"Exact_Matches: {record['exact_matches']}\")\n",
    "#     print(f\"Average Communication Loss: {record['avg_loss']:.7f}\")\n",
    "#     print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-08-17T14:35:19.237666Z",
     "iopub.status.busy": "2023-08-17T14:35:19.237522Z",
     "iopub.status.idle": "2023-08-17T14:35:19.241707Z",
     "shell.execute_reply": "2023-08-17T14:35:19.241208Z",
     "shell.execute_reply.started": "2023-08-17T14:35:19.237649Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # simplified population model, loss went up\n",
    "# class Agent:\n",
    "#     def __init__(self, model, id, optimizer=None):\n",
    "#         self.model = model\n",
    "#         self.id = id\n",
    "#         self.alive = True\n",
    "        \n",
    "#         # Assign optimizer\n",
    "#         self.optimizer = optimizer if optimizer else torch.optim.AdamW(self.model.parameters(), lr=0.001)\n",
    "\n",
    "#     def send(self, input_tensor):     \n",
    "#         # Use the encoder to encode the sentence\n",
    "#         logits, embed = self.model.encoder(input_tensor)\n",
    "#         msg = self.model.reparameterize(logits)\n",
    "#         return msg, embed, logits\n",
    "\n",
    "#     def receive(self, msg, embed):\n",
    "#         # Use the decoder to decode the message\n",
    "#         decoded_output = self.model.decoder(embed, msg)\n",
    "        \n",
    "#         # Convert the output probabilities to predicted token IDs\n",
    "#         _, predicted_ids = torch.max(decoded_output, dim=2)\n",
    "#         predicted_ids = predicted_ids.squeeze().cpu().numpy()\n",
    "        \n",
    "#         # Convert the predicted token IDs back to words\n",
    "#         decoded_sentence = ' '.join([vocab[idx] for idx in predicted_ids])\n",
    "#         return decoded_output, decoded_sentence\n",
    "\n",
    "# def simulation(num_generations=10, num_agents=100):\n",
    "#     pretrained_model_path = 'transformer_cvae.dict'\n",
    "#     population = []\n",
    "    \n",
    "#     base_model = TransformerCVAE().to(device)\n",
    "#     base_model.load_state_dict(torch.load(pretrained_model_path))\n",
    "    \n",
    "#     for i in range(num_agents):\n",
    "#         agent_model = copy.deepcopy(base_model)\n",
    "#         optimizer = torch.optim.AdamW(agent_model.parameters())\n",
    "#         population.append(Agent(agent_model, id=i, optimizer=optimizer))\n",
    "\n",
    "#     history = []\n",
    "#     scaler = GradScaler()  # Initialize the gradient scaler\n",
    "\n",
    "#     for generation in range(num_generations):\n",
    "#         print(f'Into Gen{generation+1}')\n",
    "#         record = {\n",
    "#             'generation': generation,\n",
    "#             'population_size': len(population),\n",
    "#             'successful_communications': 0,\n",
    "#             'total_communications': 0,\n",
    "#             'avg_loss': 0,\n",
    "#             'communications': [],\n",
    "#         }\n",
    "\n",
    "#         # Communication phase\n",
    "#         for sender_agent in population:\n",
    "#             agnet_avg_loss = 0\n",
    "#             for receiver_agent in population:\n",
    "#                 if sender_agent.id != receiver_agent.id:  # Ensure an agent doesn't communicate with itself\n",
    "                    \n",
    "#                     optimizer_sender = sender_agent.optimizer\n",
    "#                     optimizer_receiver = receiver_agent.optimizer\n",
    "                    \n",
    "#                     idx = torch.randint(0, len(sentences), (1,)).item()\n",
    "#                     sentence = sentences[idx]\n",
    "                    \n",
    "#                     # Tokenize and encode the sentence\n",
    "#                     encoded_sentence = tokenize_and_encode(sentence)\n",
    "#                     input_tensor = torch.tensor([encoded_sentence], dtype=torch.long).to(device)\n",
    "                    \n",
    "#                     with autocast():  # Enable autocast for FP16\n",
    "#                         msg, emb, logit = sender_agent.send(input_tensor)\n",
    "#                         d_tensor, d_sentence = receiver_agent.receive(msg, emb)\n",
    "#                         loss,_,_ = combined_loss_fn(d_tensor, input_tensor, logit, beta=0.0)\n",
    "                    \n",
    "#                     # Use the scaler to scale the loss and back-propagate the scaled gradients\n",
    "#                     scaler.scale(loss).backward()\n",
    "#                     # Gradient clipping\n",
    "#                     torch.nn.utils.clip_grad_norm_(sender_agent.model.parameters(), 1.0)\n",
    "#                     torch.nn.utils.clip_grad_norm_(receiver_agent.model.parameters(), 1.0)\n",
    "#                     # Using the agent's own optimizer\n",
    "#                     scaler.step(optimizer_sender)\n",
    "#                     scaler.step(optimizer_receiver)\n",
    "#                     scaler.update()\n",
    "\n",
    "#                     record['total_communications'] += 1\n",
    "#                     if loss.item() < 0.5:\n",
    "#                         record['successful_communications'] += 1\n",
    "#                     record['avg_loss'] += loss.item()\n",
    "#                     record['communications'].append({\n",
    "#                         'sender': sender_agent.id,\n",
    "#                         'receiver': receiver_agent.id,\n",
    "#                         'original_sentence': sentence,\n",
    "#                         'decoded_sentence': d_sentence,\n",
    "#                         'loss': loss.item(),\n",
    "#                     })\n",
    "#                     agnet_avg_loss += loss.item()\n",
    "#                     print(f'agent:{sender_agent.id},loss:{loss.item()}')\n",
    "\n",
    "#         record['avg_loss'] /= record['total_communications']\n",
    "#         print(f\"Gen[{generation+1}/{num_generations}] | Suc. Rate: {record['successful_communications']/record['total_communications']} | Avg. Loss: {record['avg_loss']}\")\n",
    "#         history.append(record)\n",
    "\n",
    "#     return history, population\n",
    "\n",
    "# # 提取句子列表\n",
    "# sentences = []\n",
    "# for batch in train_dataloader:\n",
    "#     for seq in batch:\n",
    "#         sentence = ' '.join([vocab[idx.item()] for idx in seq])\n",
    "#         sentences.append(sentence)\n",
    "\n",
    "# # Call the simulation function\n",
    "# history, population = simulation(num_generations=30, num_agents=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-08-17T14:35:19.242659Z",
     "iopub.status.busy": "2023-08-17T14:35:19.242464Z",
     "iopub.status.idle": "2023-08-17T14:35:19.246759Z",
     "shell.execute_reply": "2023-08-17T14:35:19.246249Z",
     "shell.execute_reply.started": "2023-08-17T14:35:19.242641Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Trail - rewrite the Agent model, seperated sender and receiver, load batch data\n",
    "# class Agent:\n",
    "#     def __init__(self, id):\n",
    "#         self.sender_model = TransformerEncoder().to(device)\n",
    "#         self.receiver_model = TransformerDecoder().to(device)\n",
    "#         self.id = id\n",
    "#         self.alive = True\n",
    "#         self.sender_optimizer = torch.optim.Adam(self.sender_model.parameters(), lr=1e-3)\n",
    "#         self.receiver_optimizer = torch.optim.Adam(self.receiver_model.parameters(), lr=1e-3)\n",
    "\n",
    "#     def reparameterize(self, logits):\n",
    "#         return F.gumbel_softmax(logits, tau=TAU, hard=False, dim=-1)\n",
    "    \n",
    "#     def send(self, msg):\n",
    "#         logit, emb = self.sender_model(msg)\n",
    "#         z = self.reparameterize(logit)\n",
    "#         return emb, z, logit\n",
    "    \n",
    "#     def receive(self, emb, z):\n",
    "#         decoded_output = self.receiver_model(emb, z)\n",
    "#         # Convert the output probabilities to predicted token IDs\n",
    "#         _, predicted_ids = torch.max(decoded_output, dim=2)\n",
    "#         predicted_ids = predicted_ids.squeeze().cpu().numpy()\n",
    "#         # Convert the predicted token IDs back to words\n",
    "#         decoded_sentence = ' '.join([vocab[idx] for idx in predicted_ids])\n",
    "#         return decoded_output, decoded_sentence\n",
    "            \n",
    "\n",
    "# def simulation(num_generations=10, initial_population=10):\n",
    "#     agents = []\n",
    "#     for i in range(initial_population):\n",
    "#         agents.append(Agent(id=i))\n",
    "#     history = []\n",
    "    \n",
    "#     scaler = GradScaler()  # Initialize the gradient scaler\n",
    "    \n",
    "#     for generation in range(num_generations):\n",
    "#         record = {\n",
    "#             'generation': generation,\n",
    "#             'population_size': len(agents),\n",
    "#             'population_active': sum([agent.alive for agent in agents]),\n",
    "#             'successful_communications': 0,\n",
    "#             'exact_matches': 0,\n",
    "#             'total_communications': 0,\n",
    "#             'successful_communication_rate': 0,\n",
    "#             'avg_loss': 0,\n",
    "#             'communications': [],\n",
    "#             'agents_id': [],\n",
    "#         }\n",
    "\n",
    "#         # Agents communicate\n",
    "#         for sender in agents:\n",
    "#             agnet_avg_loss = 0\n",
    "#             for receiver in agents:\n",
    "#                 if sender.id == receiver.id:\n",
    "#                     continue\n",
    "#                 if not sender.alive or not receiver.alive:\n",
    "#                     continue\n",
    "# #                 idx = torch.randint(0, len(sentences), (1,)).item()\n",
    "# #                 sentence = sentences[idx]\n",
    "                \n",
    "#                 batch_sentence = next(iter(train_dataloader))\n",
    "                \n",
    "#                 # Tokenize and encode the sentence\n",
    "#                 # encoded_sentence = tokenize_and_encode(batch_sentence)\n",
    "#                 input_tensor = torch.tensor([encoded_sentence], dtype=torch.long).to(device)\n",
    "                \n",
    "#                 with autocast():  # Enable autocast for FP16\n",
    "#                     # Sender sends the message\n",
    "#                     emb, z, logit = sender.send(input_tensor)\n",
    "#                     # Receiver receives the message\n",
    "#                     output_tensor, decoded_sentence = receiver.receive(emb, z)\n",
    "#                     # Calculate the loss\n",
    "#                     loss,_,_ = combined_loss_fn(output_tensor, input_tensor, logit, beta=0.0)\n",
    "\n",
    "#                 # Use the scaler to scale the loss and back-propagate the scaled gradients\n",
    "#                 scaler.scale(loss).backward()\n",
    "#                 # Gradient clipping\n",
    "#                 torch.nn.utils.clip_grad_norm_(sender.sender_model.parameters(), 1.0)\n",
    "#                 torch.nn.utils.clip_grad_norm_(receiver.receiver_model.parameters(), 1.0)\n",
    "#                 # Using the agent's own optimizer\n",
    "#                 scaler.step(sender.sender_optimizer)\n",
    "#                 scaler.step(receiver.receiver_optimizer)\n",
    "#                 scaler.update()\n",
    "#                 record['total_communications'] += 1\n",
    "#                 if loss.item() < 0.5:\n",
    "#                     record['successful_communications'] += 1\n",
    "#                 record['avg_loss'] += loss.item()\n",
    "#                 record['communications'].append({\n",
    "#                     'sender': sender.id,\n",
    "#                     'receiver': receiver.id,\n",
    "#                     'original_sentence': sentence,\n",
    "#                     'decoded_sentence': decoded_sentence,\n",
    "#                     'loss': loss.item(),\n",
    "#                 })\n",
    "#                 agnet_avg_loss += loss.item()\n",
    "#                 # print(f'agent:{sender.id},loss:{loss.item()}')\n",
    "\n",
    "#         record['avg_loss'] /= record['total_communications']\n",
    "#         print(f\"Gen[{generation+1}/{num_generations}] | Suc. Rate: {record['successful_communications']/record['total_communications']} | Avg. Loss: {record['avg_loss']}\")\n",
    "#         history.append(record)\n",
    "\n",
    "#     return history, agents\n",
    "\n",
    "# # 提取句子列表\n",
    "# sentences = []\n",
    "# for batch in train_dataloader:\n",
    "#     for seq in batch:\n",
    "#         sentence = ' '.join([vocab[idx.item()] for idx in seq])\n",
    "#         sentences.append(sentence)\n",
    "\n",
    "# # Call the simulation function\n",
    "# history, population = simulation(num_generations=10, initial_population=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-08-17T14:35:19.247696Z",
     "iopub.status.busy": "2023-08-17T14:35:19.247551Z",
     "iopub.status.idle": "2023-08-17T14:35:19.250078Z",
     "shell.execute_reply": "2023-08-17T14:35:19.249564Z",
     "shell.execute_reply.started": "2023-08-17T14:35:19.247679Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Print the history records\n",
    "# for record in history:\n",
    "#     print(f\"Generation: {record['generation']}\")\n",
    "#     print(f\"Population Size: {record['population_size']}\")\n",
    "#     print(f\"Average Influence: {record['avg_influence']:.2f}\")\n",
    "#     print(f\"Successful Communications: {record['successful_communications']}/{record['total_communications']}\")\n",
    "#     print(f\"Successful Communication Rate: {record['successful_communication_rate']:.7f}\")\n",
    "#     print(f\"Average Communication Loss: {record['avg_loss']:.7f}\")\n",
    "#     for comm in record['communications']:\n",
    "#         print(f\"  Agent {comm['sender']} -> Agent {comm['receiver']}:\")\n",
    "#         print(f\"    Exact Match: {comm['exact_match']}\")\n",
    "#         print(f\"    Original Sentence: {comm['original_sentence']}\")\n",
    "#         print(f\"    Decoded Sentence: {comm['decoded_sentence']}\")\n",
    "#         print(f\"    Successful: {comm['successful']}\")\n",
    "#         print(f\"    Loss: {comm['loss']:.7f}\")\n",
    "#     print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-17T14:35:19.250998Z",
     "iopub.status.busy": "2023-08-17T14:35:19.250794Z",
     "iopub.status.idle": "2023-08-17T14:35:19.257188Z",
     "shell.execute_reply": "2023-08-17T14:35:19.256707Z",
     "shell.execute_reply.started": "2023-08-17T14:35:19.250980Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This included extensive evidence for the production of high status jewellery and Phillip from the seventh'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_sentence = \"This included extensive evidence for the production of high status jewellery and moulds from the seventh\"\n",
    "test_generate = generate_text(transformer_cvae, valid_sentence, vocab)\n",
    "test_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-08-17T14:35:19.258002Z",
     "iopub.status.busy": "2023-08-17T14:35:19.257864Z",
     "iopub.status.idle": "2023-08-17T14:35:19.592356Z",
     "shell.execute_reply": "2023-08-17T14:35:19.591396Z",
     "shell.execute_reply.started": "2023-08-17T14:35:19.257986Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-298936225e4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mencoded_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_and_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoded_sentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-50780724c551>\u001b[0m in \u001b[0;36mtokenize_and_encode\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Convert tokens to integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize_and_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUNK_TOKEN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mencoded_data_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenize_and_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwikitext_sentences_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_agents_with_tsne(agents, input_tensor):\n",
    "    # 1. 获取每个代理模型的输出\n",
    "    embeddings = []\n",
    "    for agent in agents:\n",
    "        with torch.no_grad():\n",
    "            logits, _ = agent.model.encoder(input_tensor)\n",
    "            embeddings.append(logits.squeeze().cpu().numpy())\n",
    "\n",
    "    # 2. 使用t-SNE将编码降维到2D\n",
    "    embeddings_2d = TSNE(n_components=2).fit_transform(embeddings)\n",
    "\n",
    "    # 3. 在2D空间中绘制每个代理的位置\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i, coord in enumerate(embeddings_2d):\n",
    "        plt.scatter(*coord, label=f\"Agent {agents[i].id}\")\n",
    "    plt.title(\"Agents' Embeddings Visualization with t-SNE\")\n",
    "    # plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# 选择一个固定的输入\n",
    "idx = torch.randint(0, len(sentences), (1,)).item()\n",
    "sentence = sentences[idx]\n",
    "encoded_sentence = tokenize_and_encode(sentence)\n",
    "input_tensor = torch.tensor([encoded_sentence], dtype=torch.long).to(device)\n",
    "\n",
    "# 调用函数进行可视化\n",
    "visualize_agents_with_tsne(population, input_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.status.busy": "2023-08-17T14:35:19.592996Z",
     "iopub.status.idle": "2023-08-17T14:35:19.593206Z",
     "shell.execute_reply": "2023-08-17T14:35:19.593099Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_weight_distributions(agents, num_agents=5):\n",
    "    fig, axs = plt.subplots(num_agents, 6, figsize=(20, 3*num_agents))\n",
    "\n",
    "    for idx, agent in enumerate(agents[:num_agents]):\n",
    "        # Encoder Embedding\n",
    "        weights = agent.model.encoder.embedding.weight.detach().cpu().numpy()\n",
    "        axs[idx, 0].hist(weights.flatten(), bins=50, color='b', alpha=0.7)\n",
    "        axs[idx, 0].set_title(f'Agent {agent.id} - Enc Embedding')\n",
    "\n",
    "        # Encoder fc_logits\n",
    "        weights = agent.model.encoder.fc_logits.weight.detach().cpu().numpy()\n",
    "        axs[idx, 1].hist(weights.flatten(), bins=50, color='r', alpha=0.7)\n",
    "        axs[idx, 1].set_title(f'Agent {agent.id} - Enc fc_logits')\n",
    "\n",
    "        # Decoder Embedding\n",
    "        weights = agent.model.decoder.embedding.weight.detach().cpu().numpy()\n",
    "        axs[idx, 2].hist(weights.flatten(), bins=50, color='b', alpha=0.7)\n",
    "        axs[idx, 2].set_title(f'Agent {agent.id} - Dec Embedding')\n",
    "\n",
    "        # Decoder fc_out\n",
    "        weights = agent.model.decoder.fc_out.weight.detach().cpu().numpy()\n",
    "        axs[idx, 3].hist(weights.flatten(), bins=50, color='g', alpha=0.7)\n",
    "        axs[idx, 3].set_title(f'Agent {agent.id} - Dec fc_out')\n",
    "\n",
    "        # Decoder fc_z\n",
    "        weights = agent.model.decoder.fc_z.weight.detach().cpu().numpy()\n",
    "        axs[idx, 4].hist(weights.flatten(), bins=50, color='y', alpha=0.7)\n",
    "        axs[idx, 4].set_title(f'Agent {agent.id} - Dec fc_z')\n",
    "\n",
    "        # Influence\n",
    "        axs[idx, 5].bar(0, agent.influence, color='c', alpha=0.7)\n",
    "        axs[idx, 5].set_title(f'Agent {agent.id} - Influence')\n",
    "        axs[idx, 5].set_ylim(0, max([a.influence for a in agents[:num_agents]]) + 1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_weight_distributions(population)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "平均适应度变化:\n",
    "\n",
    "记录所有agents的适应度平均值，并在每个周期后绘制它。这将为您提供一个关于整体群体如何进化的视图。\n",
    "\n",
    "python\n",
    "Copy code\n",
    "avg_fitness = sum([agent.fitness for agent in agents]) / len(agents)\n",
    "最佳和最差适应度变化:\n",
    "\n",
    "跟踪并可视化最佳和最差适应度的agents。这有助于了解适应度分布的范围如何随时间变化。\n",
    "\n",
    "模型权重分布:\n",
    "\n",
    "为了了解权重如何传播，您可以使用某种可视化工具（如PCA或t-SNE）来可视化agent模型权重的分布。如果你看到权重聚集在某些区域，这可能意味着某些特定的模型结构正在变得主导。\n",
    "\n",
    "种群大小:\n",
    "\n",
    "如果你允许agents繁殖，那么绘制种群大小作为时间函数可能会很有趣。这有助于你了解种群是如何增长或收缩的，以及是否有任何爆炸性增长或大量死亡。\n",
    "\n",
    "交互效果示例:\n",
    "\n",
    "随机选择一些句子并展示agent如何对它们进行编码和解码。这有助于定性地了解agents的性能。\n",
    "\n",
    "适应度分布直方图:\n",
    "\n",
    "在每个周期或几个周期后，您可以为所有agents的适应度绘制一个直方图，以查看适应度如何分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模拟实时展示:\n",
    "a. 动态代表每个Agent:\n",
    "使用小圆圈或其他形状代表每个agent。\n",
    "使用不同的颜色或大小来表示agent的适应度或权重。\n",
    "b. 实时交互:\n",
    "当agent互相交流时，显示连线或动画效果。\n",
    "当新的agent生成或某个agent消失时，显示出现或消失的动画效果。\n",
    "c. 时间控制:\n",
    "允许用户暂停、开始或加速模拟。\n",
    "d. 参数调整:\n",
    "提供滑块或输入框，让用户调整模拟参数，如初始agent数量、交流频率、权重传播速率等。\n",
    "2. 交互式可视化:\n",
    "a. 数据视图切换:\n",
    "允许用户在不同的视图之间切换，例如适应度曲线、权重分布图、种群大小随时间变化的图等。\n",
    "b. 工具提示和详细信息:\n",
    "当用户将鼠标悬停在某个数据点或agent上时，显示详细的信息或统计数据。\n",
    "c. 参数调整:\n",
    "提供控件，允许用户修改可视化的参数。例如，查看在不同权重传播策略下的适应度曲线。\n",
    "d. 交互式教程:\n",
    "为用户提供一个交互式教程，引导他们了解如何使用可视化工具，以及他们可以从中学到什么。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
